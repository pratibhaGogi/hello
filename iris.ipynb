{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSwPQK/4mTAXCHEYPKT9HK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratibhaGogi/hello/blob/master/iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci37ygliwY6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7024fc01-ecfa-4495-f517-c0b47cdc9431"
      },
      "source": [
        "import pandas as pd #Python Data Analysis Library \n",
        "import numpy as np #Python Scientific Library \n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length','sepal_width','petal_length','petal_width','iris_class']\n",
        "dataset = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')\n",
        "dataset.info()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            "sepal_length    150 non-null float64\n",
            "sepal_width     150 non-null float64\n",
            "petal_length    150 non-null float64\n",
            "petal_width     150 non-null float64\n",
            "iris_class      150 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71p5h9r5wxxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "04fad118-417d-48f7-f757-9ace108429e8"
      },
      "source": [
        "dataset.head(6)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width   iris_class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
              "5           5.4          3.9           1.7          0.4  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfmXEGPhw3zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1a6bfab7-fa97-4397-e7cd-7d49b900372a"
      },
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhl5M8Fw-oI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "5bb32991-2a75-4979-9e90-53b4c2e2044d"
      },
      "source": [
        "y=pd.get_dummies(y)\n",
        "y.sample(7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "134            0                0               1\n",
              "0              1                0               0\n",
              "91             0                1               0\n",
              "109            0                0               1\n",
              "110            0                0               1\n",
              "17             1                0               0\n",
              "149            0                0               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qNWFJq-xEGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97acdac8-b885-4314-e4e8-366a4c584084"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(105, 4) (105, 3)\n",
            "(45, 4) (45, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JHVf3URxLxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "412c96eb-d2af-41fe-a908-34f3bf339ea6"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.52387966\n",
            "Iteration 2, loss = 2.48461735\n",
            "Iteration 3, loss = 2.43318359\n",
            "Iteration 4, loss = 2.37515938\n",
            "Iteration 5, loss = 2.31680027\n",
            "Iteration 6, loss = 2.25961590\n",
            "Iteration 7, loss = 2.20480629\n",
            "Iteration 8, loss = 2.15296286\n",
            "Iteration 9, loss = 2.10572943\n",
            "Iteration 10, loss = 2.06368019\n",
            "Iteration 11, loss = 2.02697153\n",
            "Iteration 12, loss = 1.99554491\n",
            "Iteration 13, loss = 1.96894619\n",
            "Iteration 14, loss = 1.94738852\n",
            "Iteration 15, loss = 1.93049830\n",
            "Iteration 16, loss = 1.91798372\n",
            "Iteration 17, loss = 1.90974029\n",
            "Iteration 18, loss = 1.90393534\n",
            "Iteration 19, loss = 1.89975480\n",
            "Iteration 20, loss = 1.89668268\n",
            "Iteration 21, loss = 1.89414703\n",
            "Iteration 22, loss = 1.89167646\n",
            "Iteration 23, loss = 1.88904576\n",
            "Iteration 24, loss = 1.88610538\n",
            "Iteration 25, loss = 1.88277101\n",
            "Iteration 26, loss = 1.87906017\n",
            "Iteration 27, loss = 1.87505472\n",
            "Iteration 28, loss = 1.87075903\n",
            "Iteration 29, loss = 1.86621383\n",
            "Iteration 30, loss = 1.86164388\n",
            "Iteration 31, loss = 1.85704026\n",
            "Iteration 32, loss = 1.85241927\n",
            "Iteration 33, loss = 1.84780802\n",
            "Iteration 34, loss = 1.84321149\n",
            "Iteration 35, loss = 1.83866092\n",
            "Iteration 36, loss = 1.83418363\n",
            "Iteration 37, loss = 1.82979280\n",
            "Iteration 38, loss = 1.82549976\n",
            "Iteration 39, loss = 1.82131457\n",
            "Iteration 40, loss = 1.81723075\n",
            "Iteration 41, loss = 1.81324248\n",
            "Iteration 42, loss = 1.80934559\n",
            "Iteration 43, loss = 1.80553528\n",
            "Iteration 44, loss = 1.80191832\n",
            "Iteration 45, loss = 1.79861224\n",
            "Iteration 46, loss = 1.79542565\n",
            "Iteration 47, loss = 1.79239383\n",
            "Iteration 48, loss = 1.78954250\n",
            "Iteration 49, loss = 1.78677028\n",
            "Iteration 50, loss = 1.78423966\n",
            "Iteration 51, loss = 1.78169366\n",
            "Iteration 52, loss = 1.77913466\n",
            "Iteration 53, loss = 1.77657655\n",
            "Iteration 54, loss = 1.77399752\n",
            "Iteration 55, loss = 1.77145305\n",
            "Iteration 56, loss = 1.76894958\n",
            "Iteration 57, loss = 1.76651133\n",
            "Iteration 58, loss = 1.76415630\n",
            "Iteration 59, loss = 1.76178120\n",
            "Iteration 60, loss = 1.75939042\n",
            "Iteration 61, loss = 1.75700580\n",
            "Iteration 62, loss = 1.75465380\n",
            "Iteration 63, loss = 1.75230782\n",
            "Iteration 64, loss = 1.74996759\n",
            "Iteration 65, loss = 1.74763267\n",
            "Iteration 66, loss = 1.74530237\n",
            "Iteration 67, loss = 1.74297576\n",
            "Iteration 68, loss = 1.74065167\n",
            "Iteration 69, loss = 1.73835890\n",
            "Iteration 70, loss = 1.73611508\n",
            "Iteration 71, loss = 1.73386888\n",
            "Iteration 72, loss = 1.73161884\n",
            "Iteration 73, loss = 1.72936326\n",
            "Iteration 74, loss = 1.72710032\n",
            "Iteration 75, loss = 1.72482815\n",
            "Iteration 76, loss = 1.72254488\n",
            "Iteration 77, loss = 1.72024846\n",
            "Iteration 78, loss = 1.71793710\n",
            "Iteration 79, loss = 1.71560910\n",
            "Iteration 80, loss = 1.71326449\n",
            "Iteration 81, loss = 1.71091517\n",
            "Iteration 82, loss = 1.70854789\n",
            "Iteration 83, loss = 1.70616878\n",
            "Iteration 84, loss = 1.70378124\n",
            "Iteration 85, loss = 1.70137439\n",
            "Iteration 86, loss = 1.69896699\n",
            "Iteration 87, loss = 1.69654884\n",
            "Iteration 88, loss = 1.69411047\n",
            "Iteration 89, loss = 1.69165061\n",
            "Iteration 90, loss = 1.68916779\n",
            "Iteration 91, loss = 1.68667009\n",
            "Iteration 92, loss = 1.68415219\n",
            "Iteration 93, loss = 1.68161888\n",
            "Iteration 94, loss = 1.67908158\n",
            "Iteration 95, loss = 1.67652524\n",
            "Iteration 96, loss = 1.67394645\n",
            "Iteration 97, loss = 1.67134688\n",
            "Iteration 98, loss = 1.66872534\n",
            "Iteration 99, loss = 1.66608707\n",
            "Iteration 100, loss = 1.66343406\n",
            "Iteration 101, loss = 1.66076217\n",
            "Iteration 102, loss = 1.65806468\n",
            "Iteration 103, loss = 1.65534022\n",
            "Iteration 104, loss = 1.65258883\n",
            "Iteration 105, loss = 1.64981024\n",
            "Iteration 106, loss = 1.64700415\n",
            "Iteration 107, loss = 1.64417014\n",
            "Iteration 108, loss = 1.64130784\n",
            "Iteration 109, loss = 1.63841881\n",
            "Iteration 110, loss = 1.63551891\n",
            "Iteration 111, loss = 1.63259019\n",
            "Iteration 112, loss = 1.62964578\n",
            "Iteration 113, loss = 1.62666586\n",
            "Iteration 114, loss = 1.62365738\n",
            "Iteration 115, loss = 1.62062620\n",
            "Iteration 116, loss = 1.61756790\n",
            "Iteration 117, loss = 1.61448365\n",
            "Iteration 118, loss = 1.61137236\n",
            "Iteration 119, loss = 1.60823273\n",
            "Iteration 120, loss = 1.60506886\n",
            "Iteration 121, loss = 1.60187899\n",
            "Iteration 122, loss = 1.59866142\n",
            "Iteration 123, loss = 1.59541485\n",
            "Iteration 124, loss = 1.59214104\n",
            "Iteration 125, loss = 1.58884139\n",
            "Iteration 126, loss = 1.58551254\n",
            "Iteration 127, loss = 1.58215596\n",
            "Iteration 128, loss = 1.57877463\n",
            "Iteration 129, loss = 1.57536839\n",
            "Iteration 130, loss = 1.57193377\n",
            "Iteration 131, loss = 1.56847113\n",
            "Iteration 132, loss = 1.56498747\n",
            "Iteration 133, loss = 1.56146981\n",
            "Iteration 134, loss = 1.55792517\n",
            "Iteration 135, loss = 1.55435899\n",
            "Iteration 136, loss = 1.55076497\n",
            "Iteration 137, loss = 1.54714318\n",
            "Iteration 138, loss = 1.54350626\n",
            "Iteration 139, loss = 1.53984542\n",
            "Iteration 140, loss = 1.53615995\n",
            "Iteration 141, loss = 1.53244947\n",
            "Iteration 142, loss = 1.52871590\n",
            "Iteration 143, loss = 1.52495557\n",
            "Iteration 144, loss = 1.52118242\n",
            "Iteration 145, loss = 1.51740642\n",
            "Iteration 146, loss = 1.51361746\n",
            "Iteration 147, loss = 1.50982238\n",
            "Iteration 148, loss = 1.50601109\n",
            "Iteration 149, loss = 1.50219045\n",
            "Iteration 150, loss = 1.49836802\n",
            "Iteration 151, loss = 1.49453177\n",
            "Iteration 152, loss = 1.49067883\n",
            "Iteration 153, loss = 1.48683637\n",
            "Iteration 154, loss = 1.48301541\n",
            "Iteration 155, loss = 1.47918165\n",
            "Iteration 156, loss = 1.47533408\n",
            "Iteration 157, loss = 1.47147231\n",
            "Iteration 158, loss = 1.46759904\n",
            "Iteration 159, loss = 1.46372507\n",
            "Iteration 160, loss = 1.45986323\n",
            "Iteration 161, loss = 1.45599358\n",
            "Iteration 162, loss = 1.45212497\n",
            "Iteration 163, loss = 1.44826142\n",
            "Iteration 164, loss = 1.44438524\n",
            "Iteration 165, loss = 1.44050977\n",
            "Iteration 166, loss = 1.43663076\n",
            "Iteration 167, loss = 1.43274629\n",
            "Iteration 168, loss = 1.42885312\n",
            "Iteration 169, loss = 1.42495482\n",
            "Iteration 170, loss = 1.42104490\n",
            "Iteration 171, loss = 1.41713185\n",
            "Iteration 172, loss = 1.41321822\n",
            "Iteration 173, loss = 1.40929440\n",
            "Iteration 174, loss = 1.40536555\n",
            "Iteration 175, loss = 1.40143372\n",
            "Iteration 176, loss = 1.39749973\n",
            "Iteration 177, loss = 1.39355915\n",
            "Iteration 178, loss = 1.38961474\n",
            "Iteration 179, loss = 1.38566685\n",
            "Iteration 180, loss = 1.38171547\n",
            "Iteration 181, loss = 1.37776117\n",
            "Iteration 182, loss = 1.37380691\n",
            "Iteration 183, loss = 1.36984769\n",
            "Iteration 184, loss = 1.36588675\n",
            "Iteration 185, loss = 1.36192609\n",
            "Iteration 186, loss = 1.35796639\n",
            "Iteration 187, loss = 1.35400499\n",
            "Iteration 188, loss = 1.35004393\n",
            "Iteration 189, loss = 1.34608623\n",
            "Iteration 190, loss = 1.34213505\n",
            "Iteration 191, loss = 1.33817857\n",
            "Iteration 192, loss = 1.33423188\n",
            "Iteration 193, loss = 1.33028785\n",
            "Iteration 194, loss = 1.32634367\n",
            "Iteration 195, loss = 1.32240935\n",
            "Iteration 196, loss = 1.31848131\n",
            "Iteration 197, loss = 1.31455937\n",
            "Iteration 198, loss = 1.31064465\n",
            "Iteration 199, loss = 1.30673706\n",
            "Iteration 200, loss = 1.30283789\n",
            "Iteration 201, loss = 1.29894657\n",
            "Iteration 202, loss = 1.29506416\n",
            "Iteration 203, loss = 1.29119127\n",
            "Iteration 204, loss = 1.28733095\n",
            "Iteration 205, loss = 1.28347918\n",
            "Iteration 206, loss = 1.27967462\n",
            "Iteration 207, loss = 1.27583558\n",
            "Iteration 208, loss = 1.27202279\n",
            "Iteration 209, loss = 1.26823097\n",
            "Iteration 210, loss = 1.26445423\n",
            "Iteration 211, loss = 1.26069317\n",
            "Iteration 212, loss = 1.25694620\n",
            "Iteration 213, loss = 1.25321484\n",
            "Iteration 214, loss = 1.24949950\n",
            "Iteration 215, loss = 1.24579901\n",
            "Iteration 216, loss = 1.24211527\n",
            "Iteration 217, loss = 1.23845236\n",
            "Iteration 218, loss = 1.23480660\n",
            "Iteration 219, loss = 1.23117851\n",
            "Iteration 220, loss = 1.22756814\n",
            "Iteration 221, loss = 1.22397576\n",
            "Iteration 222, loss = 1.22040135\n",
            "Iteration 223, loss = 1.21684540\n",
            "Iteration 224, loss = 1.21331515\n",
            "Iteration 225, loss = 1.20979805\n",
            "Iteration 226, loss = 1.20630054\n",
            "Iteration 227, loss = 1.20282242\n",
            "Iteration 228, loss = 1.19936766\n",
            "Iteration 229, loss = 1.19593113\n",
            "Iteration 230, loss = 1.19251958\n",
            "Iteration 231, loss = 1.18912576\n",
            "Iteration 232, loss = 1.18575417\n",
            "Iteration 233, loss = 1.18240957\n",
            "Iteration 234, loss = 1.17908113\n",
            "Iteration 235, loss = 1.17577705\n",
            "Iteration 236, loss = 1.17249561\n",
            "Iteration 237, loss = 1.16923690\n",
            "Iteration 238, loss = 1.16600089\n",
            "Iteration 239, loss = 1.16278774\n",
            "Iteration 240, loss = 1.15959771\n",
            "Iteration 241, loss = 1.15643016\n",
            "Iteration 242, loss = 1.15328544\n",
            "Iteration 243, loss = 1.15016329\n",
            "Iteration 244, loss = 1.14706735\n",
            "Iteration 245, loss = 1.14399125\n",
            "Iteration 246, loss = 1.14094335\n",
            "Iteration 247, loss = 1.13791568\n",
            "Iteration 248, loss = 1.13491380\n",
            "Iteration 249, loss = 1.13193655\n",
            "Iteration 250, loss = 1.12898222\n",
            "Iteration 251, loss = 1.12605245\n",
            "Iteration 252, loss = 1.12314635\n",
            "Iteration 253, loss = 1.12026459\n",
            "Iteration 254, loss = 1.11740687\n",
            "Iteration 255, loss = 1.11457310\n",
            "Iteration 256, loss = 1.11176281\n",
            "Iteration 257, loss = 1.10897577\n",
            "Iteration 258, loss = 1.10621175\n",
            "Iteration 259, loss = 1.10347052\n",
            "Iteration 260, loss = 1.10075190\n",
            "Iteration 261, loss = 1.09805582\n",
            "Iteration 262, loss = 1.09538240\n",
            "Iteration 263, loss = 1.09273115\n",
            "Iteration 264, loss = 1.09010225\n",
            "Iteration 265, loss = 1.08749519\n",
            "Iteration 266, loss = 1.08491019\n",
            "Iteration 267, loss = 1.08234694\n",
            "Iteration 268, loss = 1.07980559\n",
            "Iteration 269, loss = 1.07728595\n",
            "Iteration 270, loss = 1.07478762\n",
            "Iteration 271, loss = 1.07231047\n",
            "Iteration 272, loss = 1.06985436\n",
            "Iteration 273, loss = 1.06741913\n",
            "Iteration 274, loss = 1.06500462\n",
            "Iteration 275, loss = 1.06261057\n",
            "Iteration 276, loss = 1.06023682\n",
            "Iteration 277, loss = 1.05788322\n",
            "Iteration 278, loss = 1.05554956\n",
            "Iteration 279, loss = 1.05323569\n",
            "Iteration 280, loss = 1.05094149\n",
            "Iteration 281, loss = 1.04866668\n",
            "Iteration 282, loss = 1.04641967\n",
            "Iteration 283, loss = 1.04419679\n",
            "Iteration 284, loss = 1.04199563\n",
            "Iteration 285, loss = 1.03980969\n",
            "Iteration 286, loss = 1.03764852\n",
            "Iteration 287, loss = 1.03551131\n",
            "Iteration 288, loss = 1.03339586\n",
            "Iteration 289, loss = 1.03130278\n",
            "Iteration 290, loss = 1.02923177\n",
            "Iteration 291, loss = 1.02718307\n",
            "Iteration 292, loss = 1.02515601\n",
            "Iteration 293, loss = 1.02314875\n",
            "Iteration 294, loss = 1.02116136\n",
            "Iteration 295, loss = 1.01919250\n",
            "Iteration 296, loss = 1.01724132\n",
            "Iteration 297, loss = 1.01530712\n",
            "Iteration 298, loss = 1.01338941\n",
            "Iteration 299, loss = 1.01148792\n",
            "Iteration 300, loss = 1.00960252\n",
            "Iteration 301, loss = 1.00773319\n",
            "Iteration 302, loss = 1.00587993\n",
            "Iteration 303, loss = 1.00404272\n",
            "Iteration 304, loss = 1.00222150\n",
            "Iteration 305, loss = 1.00041613\n",
            "Iteration 306, loss = 0.99863930\n",
            "Iteration 307, loss = 0.99687464\n",
            "Iteration 308, loss = 0.99512329\n",
            "Iteration 309, loss = 0.99338624\n",
            "Iteration 310, loss = 0.99166537\n",
            "Iteration 311, loss = 0.98995922\n",
            "Iteration 312, loss = 0.98826889\n",
            "Iteration 313, loss = 0.98659851\n",
            "Iteration 314, loss = 0.98494227\n",
            "Iteration 315, loss = 0.98330228\n",
            "Iteration 316, loss = 0.98167633\n",
            "Iteration 317, loss = 0.98006381\n",
            "Iteration 318, loss = 0.97846427\n",
            "Iteration 319, loss = 0.97688944\n",
            "Iteration 320, loss = 0.97531476\n",
            "Iteration 321, loss = 0.97376202\n",
            "Iteration 322, loss = 0.97221824\n",
            "Iteration 323, loss = 0.97069191\n",
            "Iteration 324, loss = 0.96917398\n",
            "Iteration 325, loss = 0.96767735\n",
            "Iteration 326, loss = 0.96618494\n",
            "Iteration 327, loss = 0.96470919\n",
            "Iteration 328, loss = 0.96324641\n",
            "Iteration 329, loss = 0.96179160\n",
            "Iteration 330, loss = 0.96035014\n",
            "Iteration 331, loss = 0.95892042\n",
            "Iteration 332, loss = 0.95750772\n",
            "Iteration 333, loss = 0.95609860\n",
            "Iteration 334, loss = 0.95470298\n",
            "Iteration 335, loss = 0.95331852\n",
            "Iteration 336, loss = 0.95194464\n",
            "Iteration 337, loss = 0.95058126\n",
            "Iteration 338, loss = 0.94922839\n",
            "Iteration 339, loss = 0.94788610\n",
            "Iteration 340, loss = 0.94655368\n",
            "Iteration 341, loss = 0.94523164\n",
            "Iteration 342, loss = 0.94391927\n",
            "Iteration 343, loss = 0.94261662\n",
            "Iteration 344, loss = 0.94132336\n",
            "Iteration 345, loss = 0.94003912\n",
            "Iteration 346, loss = 0.93876393\n",
            "Iteration 347, loss = 0.93749867\n",
            "Iteration 348, loss = 0.93624101\n",
            "Iteration 349, loss = 0.93499265\n",
            "Iteration 350, loss = 0.93375341\n",
            "Iteration 351, loss = 0.93252232\n",
            "Iteration 352, loss = 0.93129970\n",
            "Iteration 353, loss = 0.93008530\n",
            "Iteration 354, loss = 0.92887917\n",
            "Iteration 355, loss = 0.92768164\n",
            "Iteration 356, loss = 0.92649119\n",
            "Iteration 357, loss = 0.92530876\n",
            "Iteration 358, loss = 0.92413402\n",
            "Iteration 359, loss = 0.92296752\n",
            "Iteration 360, loss = 0.92180764\n",
            "Iteration 361, loss = 0.92065538\n",
            "Iteration 362, loss = 0.91951025\n",
            "Iteration 363, loss = 0.91837307\n",
            "Iteration 364, loss = 0.91724181\n",
            "Iteration 365, loss = 0.91611796\n",
            "Iteration 366, loss = 0.91500107\n",
            "Iteration 367, loss = 0.91389098\n",
            "Iteration 368, loss = 0.91278734\n",
            "Iteration 369, loss = 0.91169011\n",
            "Iteration 370, loss = 0.91059966\n",
            "Iteration 371, loss = 0.90951545\n",
            "Iteration 372, loss = 0.90843868\n",
            "Iteration 373, loss = 0.90736765\n",
            "Iteration 374, loss = 0.90630252\n",
            "Iteration 375, loss = 0.90524337\n",
            "Iteration 376, loss = 0.90419075\n",
            "Iteration 377, loss = 0.90314597\n",
            "Iteration 378, loss = 0.90210492\n",
            "Iteration 379, loss = 0.90107019\n",
            "Iteration 380, loss = 0.90004271\n",
            "Iteration 381, loss = 0.89902035\n",
            "Iteration 382, loss = 0.89800309\n",
            "Iteration 383, loss = 0.89699365\n",
            "Iteration 384, loss = 0.89598793\n",
            "Iteration 385, loss = 0.89498805\n",
            "Iteration 386, loss = 0.89399397\n",
            "Iteration 387, loss = 0.89300495\n",
            "Iteration 388, loss = 0.89202201\n",
            "Iteration 389, loss = 0.89104364\n",
            "Iteration 390, loss = 0.89007029\n",
            "Iteration 391, loss = 0.88910287\n",
            "Iteration 392, loss = 0.88813960\n",
            "Iteration 393, loss = 0.88718144\n",
            "Iteration 394, loss = 0.88622803\n",
            "Iteration 395, loss = 0.88527998\n",
            "Iteration 396, loss = 0.88433588\n",
            "Iteration 397, loss = 0.88339712\n",
            "Iteration 398, loss = 0.88246256\n",
            "Iteration 399, loss = 0.88153251\n",
            "Iteration 400, loss = 0.88060761\n",
            "Iteration 401, loss = 0.87968626\n",
            "Iteration 402, loss = 0.87877006\n",
            "Iteration 403, loss = 0.87785782\n",
            "Iteration 404, loss = 0.87694984\n",
            "Iteration 405, loss = 0.87604664\n",
            "Iteration 406, loss = 0.87514698\n",
            "Iteration 407, loss = 0.87425193\n",
            "Iteration 408, loss = 0.87336095\n",
            "Iteration 409, loss = 0.87247383\n",
            "Iteration 410, loss = 0.87159091\n",
            "Iteration 411, loss = 0.87071198\n",
            "Iteration 412, loss = 0.86983678\n",
            "Iteration 413, loss = 0.86896592\n",
            "Iteration 414, loss = 0.86809841\n",
            "Iteration 415, loss = 0.86723486\n",
            "Iteration 416, loss = 0.86637549\n",
            "Iteration 417, loss = 0.86551946\n",
            "Iteration 418, loss = 0.86466704\n",
            "Iteration 419, loss = 0.86381847\n",
            "Iteration 420, loss = 0.86297350\n",
            "Iteration 421, loss = 0.86213198\n",
            "Iteration 422, loss = 0.86129422\n",
            "Iteration 423, loss = 0.86045991\n",
            "Iteration 424, loss = 0.85962897\n",
            "Iteration 425, loss = 0.85880141\n",
            "Iteration 426, loss = 0.85797772\n",
            "Iteration 427, loss = 0.85715683\n",
            "Iteration 428, loss = 0.85633946\n",
            "Iteration 429, loss = 0.85552545\n",
            "Iteration 430, loss = 0.85471493\n",
            "Iteration 431, loss = 0.85390789\n",
            "Iteration 432, loss = 0.85310378\n",
            "Iteration 433, loss = 0.85230295\n",
            "Iteration 434, loss = 0.85150567\n",
            "Iteration 435, loss = 0.85071113\n",
            "Iteration 436, loss = 0.84991958\n",
            "Iteration 437, loss = 0.84913147\n",
            "Iteration 438, loss = 0.84834607\n",
            "Iteration 439, loss = 0.84756368\n",
            "Iteration 440, loss = 0.84678468\n",
            "Iteration 441, loss = 0.84600822\n",
            "Iteration 442, loss = 0.84523473\n",
            "Iteration 443, loss = 0.84446407\n",
            "Iteration 444, loss = 0.84369698\n",
            "Iteration 445, loss = 0.84293173\n",
            "Iteration 446, loss = 0.84216969\n",
            "Iteration 447, loss = 0.84141061\n",
            "Iteration 448, loss = 0.84065431\n",
            "Iteration 449, loss = 0.83990103\n",
            "Iteration 450, loss = 0.83915027\n",
            "Iteration 451, loss = 0.83840184\n",
            "Iteration 452, loss = 0.83765709\n",
            "Iteration 453, loss = 0.83691503\n",
            "Iteration 454, loss = 0.83617563\n",
            "Iteration 455, loss = 0.83543888\n",
            "Iteration 456, loss = 0.83470476\n",
            "Iteration 457, loss = 0.83397364\n",
            "Iteration 458, loss = 0.83324471\n",
            "Iteration 459, loss = 0.83251884\n",
            "Iteration 460, loss = 0.83179501\n",
            "Iteration 461, loss = 0.83107458\n",
            "Iteration 462, loss = 0.83035585\n",
            "Iteration 463, loss = 0.82964013\n",
            "Iteration 464, loss = 0.82892686\n",
            "Iteration 465, loss = 0.82821602\n",
            "Iteration 466, loss = 0.82750760\n",
            "Iteration 467, loss = 0.82680164\n",
            "Iteration 468, loss = 0.82609821\n",
            "Iteration 469, loss = 0.82539703\n",
            "Iteration 470, loss = 0.82469817\n",
            "Iteration 471, loss = 0.82400159\n",
            "Iteration 472, loss = 0.82330729\n",
            "Iteration 473, loss = 0.82261526\n",
            "Iteration 474, loss = 0.82192584\n",
            "Iteration 475, loss = 0.82123842\n",
            "Iteration 476, loss = 0.82055320\n",
            "Iteration 477, loss = 0.81987014\n",
            "Iteration 478, loss = 0.81918960\n",
            "Iteration 479, loss = 0.81851092\n",
            "Iteration 480, loss = 0.81783447\n",
            "Iteration 481, loss = 0.81716010\n",
            "Iteration 482, loss = 0.81648843\n",
            "Iteration 483, loss = 0.81581808\n",
            "Iteration 484, loss = 0.81515010\n",
            "Iteration 485, loss = 0.81448448\n",
            "Iteration 486, loss = 0.81382073\n",
            "Iteration 487, loss = 0.81315897\n",
            "Iteration 488, loss = 0.81249915\n",
            "Iteration 489, loss = 0.81184193\n",
            "Iteration 490, loss = 0.81118593\n",
            "Iteration 491, loss = 0.81053232\n",
            "Iteration 492, loss = 0.80988090\n",
            "Iteration 493, loss = 0.80923114\n",
            "Iteration 494, loss = 0.80858319\n",
            "Iteration 495, loss = 0.80793770\n",
            "Iteration 496, loss = 0.80729346\n",
            "Iteration 497, loss = 0.80665171\n",
            "Iteration 498, loss = 0.80601155\n",
            "Iteration 499, loss = 0.80537318\n",
            "Iteration 500, loss = 0.80473718\n",
            "Iteration 501, loss = 0.80410236\n",
            "Iteration 502, loss = 0.80346991\n",
            "Iteration 503, loss = 0.80283905\n",
            "Iteration 504, loss = 0.80220982\n",
            "Iteration 505, loss = 0.80158312\n",
            "Iteration 506, loss = 0.80095731\n",
            "Iteration 507, loss = 0.80033400\n",
            "Iteration 508, loss = 0.79971195\n",
            "Iteration 509, loss = 0.79909164\n",
            "Iteration 510, loss = 0.79847340\n",
            "Iteration 511, loss = 0.79785659\n",
            "Iteration 512, loss = 0.79724179\n",
            "Iteration 513, loss = 0.79662837\n",
            "Iteration 514, loss = 0.79601698\n",
            "Iteration 515, loss = 0.79540692\n",
            "Iteration 516, loss = 0.79479886\n",
            "Iteration 517, loss = 0.79419217\n",
            "Iteration 518, loss = 0.79358729\n",
            "Iteration 519, loss = 0.79298403\n",
            "Iteration 520, loss = 0.79238217\n",
            "Iteration 521, loss = 0.79178242\n",
            "Iteration 522, loss = 0.79118369\n",
            "Iteration 523, loss = 0.79058702\n",
            "Iteration 524, loss = 0.78999138\n",
            "Iteration 525, loss = 0.78939796\n",
            "Iteration 526, loss = 0.78880548\n",
            "Iteration 527, loss = 0.78821486\n",
            "Iteration 528, loss = 0.78762589\n",
            "Iteration 529, loss = 0.78703811\n",
            "Iteration 530, loss = 0.78645200\n",
            "Iteration 531, loss = 0.78586745\n",
            "Iteration 532, loss = 0.78528413\n",
            "Iteration 533, loss = 0.78470280\n",
            "Iteration 534, loss = 0.78412232\n",
            "Iteration 535, loss = 0.78354358\n",
            "Iteration 536, loss = 0.78296653\n",
            "Iteration 537, loss = 0.78239059\n",
            "Iteration 538, loss = 0.78181597\n",
            "Iteration 539, loss = 0.78124318\n",
            "Iteration 540, loss = 0.78067137\n",
            "Iteration 541, loss = 0.78010101\n",
            "Iteration 542, loss = 0.77953255\n",
            "Iteration 543, loss = 0.77896483\n",
            "Iteration 544, loss = 0.77839864\n",
            "Iteration 545, loss = 0.77783393\n",
            "Iteration 546, loss = 0.77727068\n",
            "Iteration 547, loss = 0.77670856\n",
            "Iteration 548, loss = 0.77614771\n",
            "Iteration 549, loss = 0.77558812\n",
            "Iteration 550, loss = 0.77503043\n",
            "Iteration 551, loss = 0.77447323\n",
            "Iteration 552, loss = 0.77391763\n",
            "Iteration 553, loss = 0.77336333\n",
            "Iteration 554, loss = 0.77281055\n",
            "Iteration 555, loss = 0.77225881\n",
            "Iteration 556, loss = 0.77170827\n",
            "Iteration 557, loss = 0.77115893\n",
            "Iteration 558, loss = 0.77061080\n",
            "Iteration 559, loss = 0.77006388\n",
            "Iteration 560, loss = 0.76951879\n",
            "Iteration 561, loss = 0.76897396\n",
            "Iteration 562, loss = 0.76843075\n",
            "Iteration 563, loss = 0.76788870\n",
            "Iteration 564, loss = 0.76734782\n",
            "Iteration 565, loss = 0.76680821\n",
            "Iteration 566, loss = 0.76626977\n",
            "Iteration 567, loss = 0.76573243\n",
            "Iteration 568, loss = 0.76519620\n",
            "Iteration 569, loss = 0.76466109\n",
            "Iteration 570, loss = 0.76412709\n",
            "Iteration 571, loss = 0.76359420\n",
            "Iteration 572, loss = 0.76306242\n",
            "Iteration 573, loss = 0.76253173\n",
            "Iteration 574, loss = 0.76200215\n",
            "Iteration 575, loss = 0.76147364\n",
            "Iteration 576, loss = 0.76094622\n",
            "Iteration 577, loss = 0.76042008\n",
            "Iteration 578, loss = 0.75989463\n",
            "Iteration 579, loss = 0.75937041\n",
            "Iteration 580, loss = 0.75884721\n",
            "Iteration 581, loss = 0.75832504\n",
            "Iteration 582, loss = 0.75780388\n",
            "Iteration 583, loss = 0.75728373\n",
            "Iteration 584, loss = 0.75676458\n",
            "Iteration 585, loss = 0.75624643\n",
            "Iteration 586, loss = 0.75572927\n",
            "Iteration 587, loss = 0.75521325\n",
            "Iteration 588, loss = 0.75469970\n",
            "Iteration 589, loss = 0.75418716\n",
            "Iteration 590, loss = 0.75367562\n",
            "Iteration 591, loss = 0.75316506\n",
            "Iteration 592, loss = 0.75265548\n",
            "Iteration 593, loss = 0.75214687\n",
            "Iteration 594, loss = 0.75163921\n",
            "Iteration 595, loss = 0.75113324\n",
            "Iteration 596, loss = 0.75062813\n",
            "Iteration 597, loss = 0.75012386\n",
            "Iteration 598, loss = 0.74962083\n",
            "Iteration 599, loss = 0.74911800\n",
            "Iteration 600, loss = 0.74861634\n",
            "Iteration 601, loss = 0.74811551\n",
            "Iteration 602, loss = 0.74761551\n",
            "Iteration 603, loss = 0.74711634\n",
            "Iteration 604, loss = 0.74661799\n",
            "Iteration 605, loss = 0.74612046\n",
            "Iteration 606, loss = 0.74562375\n",
            "Iteration 607, loss = 0.74512825\n",
            "Iteration 608, loss = 0.74463304\n",
            "Iteration 609, loss = 0.74413888\n",
            "Iteration 610, loss = 0.74364550\n",
            "Iteration 611, loss = 0.74315290\n",
            "Iteration 612, loss = 0.74266122\n",
            "Iteration 613, loss = 0.74217083\n",
            "Iteration 614, loss = 0.74168099\n",
            "Iteration 615, loss = 0.74119186\n",
            "Iteration 616, loss = 0.74070344\n",
            "Iteration 617, loss = 0.74021573\n",
            "Iteration 618, loss = 0.73972916\n",
            "Iteration 619, loss = 0.73924302\n",
            "Iteration 620, loss = 0.73875765\n",
            "Iteration 621, loss = 0.73827326\n",
            "Iteration 622, loss = 0.73778945\n",
            "Iteration 623, loss = 0.73730632\n",
            "Iteration 624, loss = 0.73682396\n",
            "Iteration 625, loss = 0.73634264\n",
            "Iteration 626, loss = 0.73586170\n",
            "Iteration 627, loss = 0.73538142\n",
            "Iteration 628, loss = 0.73490211\n",
            "Iteration 629, loss = 0.73442326\n",
            "Iteration 630, loss = 0.73394507\n",
            "Iteration 631, loss = 0.73346794\n",
            "Iteration 632, loss = 0.73299108\n",
            "Iteration 633, loss = 0.73251483\n",
            "Iteration 634, loss = 0.73203987\n",
            "Iteration 635, loss = 0.73156480\n",
            "Iteration 636, loss = 0.73109066\n",
            "Iteration 637, loss = 0.73061753\n",
            "Iteration 638, loss = 0.73014454\n",
            "Iteration 639, loss = 0.72967212\n",
            "Iteration 640, loss = 0.72920084\n",
            "Iteration 641, loss = 0.72872965\n",
            "Iteration 642, loss = 0.72825951\n",
            "Iteration 643, loss = 0.72778942\n",
            "Iteration 644, loss = 0.72732060\n",
            "Iteration 645, loss = 0.72685172\n",
            "Iteration 646, loss = 0.72638384\n",
            "Iteration 647, loss = 0.72591644\n",
            "Iteration 648, loss = 0.72544929\n",
            "Iteration 649, loss = 0.72498352\n",
            "Iteration 650, loss = 0.72451744\n",
            "Iteration 651, loss = 0.72405240\n",
            "Iteration 652, loss = 0.72358742\n",
            "Iteration 653, loss = 0.72312376\n",
            "Iteration 654, loss = 0.72265978\n",
            "Iteration 655, loss = 0.72219705\n",
            "Iteration 656, loss = 0.72173421\n",
            "Iteration 657, loss = 0.72127233\n",
            "Iteration 658, loss = 0.72081047\n",
            "Iteration 659, loss = 0.72034990\n",
            "Iteration 660, loss = 0.71989013\n",
            "Iteration 661, loss = 0.71943136\n",
            "Iteration 662, loss = 0.71897236\n",
            "Iteration 663, loss = 0.71851553\n",
            "Iteration 664, loss = 0.71805967\n",
            "Iteration 665, loss = 0.71760459\n",
            "Iteration 666, loss = 0.71715028\n",
            "Iteration 667, loss = 0.71669649\n",
            "Iteration 668, loss = 0.71624329\n",
            "Iteration 669, loss = 0.71579069\n",
            "Iteration 670, loss = 0.71533859\n",
            "Iteration 671, loss = 0.71488701\n",
            "Iteration 672, loss = 0.71443594\n",
            "Iteration 673, loss = 0.71398541\n",
            "Iteration 674, loss = 0.71353531\n",
            "Iteration 675, loss = 0.71308566\n",
            "Iteration 676, loss = 0.71263650\n",
            "Iteration 677, loss = 0.71218776\n",
            "Iteration 678, loss = 0.71173945\n",
            "Iteration 679, loss = 0.71129156\n",
            "Iteration 680, loss = 0.71084410\n",
            "Iteration 681, loss = 0.71039701\n",
            "Iteration 682, loss = 0.70995030\n",
            "Iteration 683, loss = 0.70950399\n",
            "Iteration 684, loss = 0.70905805\n",
            "Iteration 685, loss = 0.70861246\n",
            "Iteration 686, loss = 0.70816722\n",
            "Iteration 687, loss = 0.70772234\n",
            "Iteration 688, loss = 0.70727779\n",
            "Iteration 689, loss = 0.70683356\n",
            "Iteration 690, loss = 0.70638964\n",
            "Iteration 691, loss = 0.70594605\n",
            "Iteration 692, loss = 0.70550278\n",
            "Iteration 693, loss = 0.70505979\n",
            "Iteration 694, loss = 0.70461709\n",
            "Iteration 695, loss = 0.70417466\n",
            "Iteration 696, loss = 0.70373257\n",
            "Iteration 697, loss = 0.70329068\n",
            "Iteration 698, loss = 0.70284908\n",
            "Iteration 699, loss = 0.70240773\n",
            "Iteration 700, loss = 0.70196666\n",
            "Iteration 701, loss = 0.70152581\n",
            "Iteration 702, loss = 0.70108521\n",
            "Iteration 703, loss = 0.70064483\n",
            "Iteration 704, loss = 0.70020467\n",
            "Iteration 705, loss = 0.69976475\n",
            "Iteration 706, loss = 0.69932505\n",
            "Iteration 707, loss = 0.69888555\n",
            "Iteration 708, loss = 0.69844624\n",
            "Iteration 709, loss = 0.69800712\n",
            "Iteration 710, loss = 0.69756820\n",
            "Iteration 711, loss = 0.69712950\n",
            "Iteration 712, loss = 0.69669092\n",
            "Iteration 713, loss = 0.69625254\n",
            "Iteration 714, loss = 0.69581432\n",
            "Iteration 715, loss = 0.69537626\n",
            "Iteration 716, loss = 0.69493837\n",
            "Iteration 717, loss = 0.69450063\n",
            "Iteration 718, loss = 0.69406303\n",
            "Iteration 719, loss = 0.69362557\n",
            "Iteration 720, loss = 0.69318892\n",
            "Iteration 721, loss = 0.69275144\n",
            "Iteration 722, loss = 0.69231502\n",
            "Iteration 723, loss = 0.69187840\n",
            "Iteration 724, loss = 0.69144191\n",
            "Iteration 725, loss = 0.69100597\n",
            "Iteration 726, loss = 0.69057038\n",
            "Iteration 727, loss = 0.69013477\n",
            "Iteration 728, loss = 0.68969932\n",
            "Iteration 729, loss = 0.68926408\n",
            "Iteration 730, loss = 0.68883166\n",
            "Iteration 731, loss = 0.68839718\n",
            "Iteration 732, loss = 0.68796221\n",
            "Iteration 733, loss = 0.68752805\n",
            "Iteration 734, loss = 0.68709354\n",
            "Iteration 735, loss = 0.68666033\n",
            "Iteration 736, loss = 0.68622677\n",
            "Iteration 737, loss = 0.68579313\n",
            "Iteration 738, loss = 0.68535944\n",
            "Iteration 739, loss = 0.68492659\n",
            "Iteration 740, loss = 0.68449286\n",
            "Iteration 741, loss = 0.68406007\n",
            "Iteration 742, loss = 0.68362694\n",
            "Iteration 743, loss = 0.68319386\n",
            "Iteration 744, loss = 0.68276115\n",
            "Iteration 745, loss = 0.68232835\n",
            "Iteration 746, loss = 0.68189542\n",
            "Iteration 747, loss = 0.68146314\n",
            "Iteration 748, loss = 0.68103026\n",
            "Iteration 749, loss = 0.68059766\n",
            "Iteration 750, loss = 0.68016557\n",
            "Iteration 751, loss = 0.67973295\n",
            "Iteration 752, loss = 0.67930028\n",
            "Iteration 753, loss = 0.67886834\n",
            "Iteration 754, loss = 0.67843579\n",
            "Iteration 755, loss = 0.67800392\n",
            "Iteration 756, loss = 0.67757152\n",
            "Iteration 757, loss = 0.67713966\n",
            "Iteration 758, loss = 0.67670759\n",
            "Iteration 759, loss = 0.67627525\n",
            "Iteration 760, loss = 0.67584389\n",
            "Iteration 761, loss = 0.67541160\n",
            "Iteration 762, loss = 0.67497952\n",
            "Iteration 763, loss = 0.67454754\n",
            "Iteration 764, loss = 0.67411560\n",
            "Iteration 765, loss = 0.67368376\n",
            "Iteration 766, loss = 0.67325174\n",
            "Iteration 767, loss = 0.67282010\n",
            "Iteration 768, loss = 0.67238777\n",
            "Iteration 769, loss = 0.67195641\n",
            "Iteration 770, loss = 0.67152397\n",
            "Iteration 771, loss = 0.67109253\n",
            "Iteration 772, loss = 0.67066013\n",
            "Iteration 773, loss = 0.67022811\n",
            "Iteration 774, loss = 0.66979570\n",
            "Iteration 775, loss = 0.66936403\n",
            "Iteration 776, loss = 0.66893128\n",
            "Iteration 777, loss = 0.66849974\n",
            "Iteration 778, loss = 0.66806711\n",
            "Iteration 779, loss = 0.66763433\n",
            "Iteration 780, loss = 0.66720219\n",
            "Iteration 781, loss = 0.66676930\n",
            "Iteration 782, loss = 0.66633713\n",
            "Iteration 783, loss = 0.66590402\n",
            "Iteration 784, loss = 0.66547189\n",
            "Iteration 785, loss = 0.66503851\n",
            "Iteration 786, loss = 0.66460607\n",
            "Iteration 787, loss = 0.66417284\n",
            "Iteration 788, loss = 0.66373941\n",
            "Iteration 789, loss = 0.66330660\n",
            "Iteration 790, loss = 0.66287291\n",
            "Iteration 791, loss = 0.66244005\n",
            "Iteration 792, loss = 0.66200586\n",
            "Iteration 793, loss = 0.66157292\n",
            "Iteration 794, loss = 0.66113899\n",
            "Iteration 795, loss = 0.66070471\n",
            "Iteration 796, loss = 0.66027097\n",
            "Iteration 797, loss = 0.65983641\n",
            "Iteration 798, loss = 0.65940240\n",
            "Iteration 799, loss = 0.65896790\n",
            "Iteration 800, loss = 0.65853311\n",
            "Iteration 801, loss = 0.65809873\n",
            "Iteration 802, loss = 0.65766366\n",
            "Iteration 803, loss = 0.65722841\n",
            "Iteration 804, loss = 0.65679379\n",
            "Iteration 805, loss = 0.65635804\n",
            "Iteration 806, loss = 0.65592234\n",
            "Iteration 807, loss = 0.65548735\n",
            "Iteration 808, loss = 0.65505101\n",
            "Iteration 809, loss = 0.65461483\n",
            "Iteration 810, loss = 0.65417922\n",
            "Iteration 811, loss = 0.65374251\n",
            "Iteration 812, loss = 0.65330583\n",
            "Iteration 813, loss = 0.65286924\n",
            "Iteration 814, loss = 0.65243244\n",
            "Iteration 815, loss = 0.65199522\n",
            "Iteration 816, loss = 0.65155769\n",
            "Iteration 817, loss = 0.65112052\n",
            "Iteration 818, loss = 0.65068260\n",
            "Iteration 819, loss = 0.65024455\n",
            "Iteration 820, loss = 0.64980630\n",
            "Iteration 821, loss = 0.64936837\n",
            "Iteration 822, loss = 0.64892975\n",
            "Iteration 823, loss = 0.64849082\n",
            "Iteration 824, loss = 0.64805159\n",
            "Iteration 825, loss = 0.64761272\n",
            "Iteration 826, loss = 0.64717301\n",
            "Iteration 827, loss = 0.64673324\n",
            "Iteration 828, loss = 0.64629316\n",
            "Iteration 829, loss = 0.64585334\n",
            "Iteration 830, loss = 0.64541279\n",
            "Iteration 831, loss = 0.64497211\n",
            "Iteration 832, loss = 0.64453110\n",
            "Iteration 833, loss = 0.64408980\n",
            "Iteration 834, loss = 0.64364861\n",
            "Iteration 835, loss = 0.64320690\n",
            "Iteration 836, loss = 0.64276495\n",
            "Iteration 837, loss = 0.64232268\n",
            "Iteration 838, loss = 0.64188011\n",
            "Iteration 839, loss = 0.64143724\n",
            "Iteration 840, loss = 0.64099481\n",
            "Iteration 841, loss = 0.64055111\n",
            "Iteration 842, loss = 0.64010755\n",
            "Iteration 843, loss = 0.63966367\n",
            "Iteration 844, loss = 0.63921947\n",
            "Iteration 845, loss = 0.63877497\n",
            "Iteration 846, loss = 0.63833034\n",
            "Iteration 847, loss = 0.63788547\n",
            "Iteration 848, loss = 0.63744020\n",
            "Iteration 849, loss = 0.63699460\n",
            "Iteration 850, loss = 0.63654866\n",
            "Iteration 851, loss = 0.63610241\n",
            "Iteration 852, loss = 0.63565584\n",
            "Iteration 853, loss = 0.63520896\n",
            "Iteration 854, loss = 0.63476178\n",
            "Iteration 855, loss = 0.63431428\n",
            "Iteration 856, loss = 0.63386699\n",
            "Iteration 857, loss = 0.63341856\n",
            "Iteration 858, loss = 0.63297019\n",
            "Iteration 859, loss = 0.63252148\n",
            "Iteration 860, loss = 0.63207242\n",
            "Iteration 861, loss = 0.63162301\n",
            "Iteration 862, loss = 0.63117327\n",
            "Iteration 863, loss = 0.63072319\n",
            "Iteration 864, loss = 0.63027277\n",
            "Iteration 865, loss = 0.62982201\n",
            "Iteration 866, loss = 0.62937092\n",
            "Iteration 867, loss = 0.62891948\n",
            "Iteration 868, loss = 0.62846770\n",
            "Iteration 869, loss = 0.62801558\n",
            "Iteration 870, loss = 0.62756310\n",
            "Iteration 871, loss = 0.62711027\n",
            "Iteration 872, loss = 0.62665709\n",
            "Iteration 873, loss = 0.62620355\n",
            "Iteration 874, loss = 0.62574965\n",
            "Iteration 875, loss = 0.62529538\n",
            "Iteration 876, loss = 0.62484075\n",
            "Iteration 877, loss = 0.62438576\n",
            "Iteration 878, loss = 0.62393039\n",
            "Iteration 879, loss = 0.62347465\n",
            "Iteration 880, loss = 0.62301853\n",
            "Iteration 881, loss = 0.62256203\n",
            "Iteration 882, loss = 0.62210515\n",
            "Iteration 883, loss = 0.62164788\n",
            "Iteration 884, loss = 0.62119022\n",
            "Iteration 885, loss = 0.62073217\n",
            "Iteration 886, loss = 0.62027372\n",
            "Iteration 887, loss = 0.61981488\n",
            "Iteration 888, loss = 0.61935563\n",
            "Iteration 889, loss = 0.61889598\n",
            "Iteration 890, loss = 0.61843593\n",
            "Iteration 891, loss = 0.61797546\n",
            "Iteration 892, loss = 0.61751458\n",
            "Iteration 893, loss = 0.61705328\n",
            "Iteration 894, loss = 0.61659157\n",
            "Iteration 895, loss = 0.61612943\n",
            "Iteration 896, loss = 0.61566687\n",
            "Iteration 897, loss = 0.61520388\n",
            "Iteration 898, loss = 0.61474047\n",
            "Iteration 899, loss = 0.61427662\n",
            "Iteration 900, loss = 0.61381234\n",
            "Iteration 901, loss = 0.61334762\n",
            "Iteration 902, loss = 0.61288408\n",
            "Iteration 903, loss = 0.61241747\n",
            "Iteration 904, loss = 0.61195221\n",
            "Iteration 905, loss = 0.61148624\n",
            "Iteration 906, loss = 0.61101971\n",
            "Iteration 907, loss = 0.61055264\n",
            "Iteration 908, loss = 0.61008505\n",
            "Iteration 909, loss = 0.60961696\n",
            "Iteration 910, loss = 0.60914957\n",
            "Iteration 911, loss = 0.60868014\n",
            "Iteration 912, loss = 0.60821101\n",
            "Iteration 913, loss = 0.60774208\n",
            "Iteration 914, loss = 0.60727219\n",
            "Iteration 915, loss = 0.60680176\n",
            "Iteration 916, loss = 0.60633089\n",
            "Iteration 917, loss = 0.60585955\n",
            "Iteration 918, loss = 0.60538859\n",
            "Iteration 919, loss = 0.60491657\n",
            "Iteration 920, loss = 0.60444408\n",
            "Iteration 921, loss = 0.60397100\n",
            "Iteration 922, loss = 0.60349883\n",
            "Iteration 923, loss = 0.60302492\n",
            "Iteration 924, loss = 0.60255088\n",
            "Iteration 925, loss = 0.60207658\n",
            "Iteration 926, loss = 0.60160183\n",
            "Iteration 927, loss = 0.60112661\n",
            "Iteration 928, loss = 0.60065067\n",
            "Iteration 929, loss = 0.60017512\n",
            "Iteration 930, loss = 0.59969851\n",
            "Iteration 931, loss = 0.59922128\n",
            "Iteration 932, loss = 0.59874446\n",
            "Iteration 933, loss = 0.59826653\n",
            "Iteration 934, loss = 0.59778851\n",
            "Iteration 935, loss = 0.59730944\n",
            "Iteration 936, loss = 0.59683008\n",
            "Iteration 937, loss = 0.59635138\n",
            "Iteration 938, loss = 0.59587067\n",
            "Iteration 939, loss = 0.59539046\n",
            "Iteration 940, loss = 0.59490980\n",
            "Iteration 941, loss = 0.59442819\n",
            "Iteration 942, loss = 0.59394653\n",
            "Iteration 943, loss = 0.59346427\n",
            "Iteration 944, loss = 0.59298141\n",
            "Iteration 945, loss = 0.59249876\n",
            "Iteration 946, loss = 0.59201476\n",
            "Iteration 947, loss = 0.59153126\n",
            "Iteration 948, loss = 0.59104610\n",
            "Iteration 949, loss = 0.59056311\n",
            "Iteration 950, loss = 0.59007712\n",
            "Iteration 951, loss = 0.58959096\n",
            "Iteration 952, loss = 0.58910395\n",
            "Iteration 953, loss = 0.58861986\n",
            "Iteration 954, loss = 0.58813253\n",
            "Iteration 955, loss = 0.58764421\n",
            "Iteration 956, loss = 0.58715559\n",
            "Iteration 957, loss = 0.58666712\n",
            "Iteration 958, loss = 0.58617912\n",
            "Iteration 959, loss = 0.58568938\n",
            "Iteration 960, loss = 0.58520031\n",
            "Iteration 961, loss = 0.58470982\n",
            "Iteration 962, loss = 0.58421973\n",
            "Iteration 963, loss = 0.58372841\n",
            "Iteration 964, loss = 0.58323739\n",
            "Iteration 965, loss = 0.58274512\n",
            "Iteration 966, loss = 0.58225330\n",
            "Iteration 967, loss = 0.58175992\n",
            "Iteration 968, loss = 0.58126744\n",
            "Iteration 969, loss = 0.58077318\n",
            "Iteration 970, loss = 0.58027973\n",
            "Iteration 971, loss = 0.57978482\n",
            "Iteration 972, loss = 0.57929043\n",
            "Iteration 973, loss = 0.57879436\n",
            "Iteration 974, loss = 0.57829899\n",
            "Iteration 975, loss = 0.57780230\n",
            "Iteration 976, loss = 0.57730578\n",
            "Iteration 977, loss = 0.57680866\n",
            "Iteration 978, loss = 0.57631127\n",
            "Iteration 979, loss = 0.57581392\n",
            "Iteration 980, loss = 0.57531476\n",
            "Iteration 981, loss = 0.57481594\n",
            "Iteration 982, loss = 0.57431694\n",
            "Iteration 983, loss = 0.57381640\n",
            "Iteration 984, loss = 0.57331690\n",
            "Iteration 985, loss = 0.57281582\n",
            "Iteration 986, loss = 0.57231449\n",
            "Iteration 987, loss = 0.57181297\n",
            "Iteration 988, loss = 0.57131021\n",
            "Iteration 989, loss = 0.57080961\n",
            "Iteration 990, loss = 0.57030517\n",
            "Iteration 991, loss = 0.56980132\n",
            "Iteration 992, loss = 0.56929842\n",
            "Iteration 993, loss = 0.56879393\n",
            "Iteration 994, loss = 0.56828881\n",
            "Iteration 995, loss = 0.56778303\n",
            "Iteration 996, loss = 0.56727771\n",
            "Iteration 997, loss = 0.56677155\n",
            "Iteration 998, loss = 0.56626462\n",
            "Iteration 999, loss = 0.56575799\n",
            "Iteration 1000, loss = 0.56524995\n",
            "Iteration 1001, loss = 0.56474228\n",
            "Iteration 1002, loss = 0.56423473\n",
            "Iteration 1003, loss = 0.56372755\n",
            "Iteration 1004, loss = 0.56321669\n",
            "Iteration 1005, loss = 0.56270964\n",
            "Iteration 1006, loss = 0.56220016\n",
            "Iteration 1007, loss = 0.56169143\n",
            "Iteration 1008, loss = 0.56118289\n",
            "Iteration 1009, loss = 0.56067372\n",
            "Iteration 1010, loss = 0.56016412\n",
            "Iteration 1011, loss = 0.55965468\n",
            "Iteration 1012, loss = 0.55914455\n",
            "Iteration 1013, loss = 0.55863404\n",
            "Iteration 1014, loss = 0.55812337\n",
            "Iteration 1015, loss = 0.55761241\n",
            "Iteration 1016, loss = 0.55710158\n",
            "Iteration 1017, loss = 0.55658983\n",
            "Iteration 1018, loss = 0.55607776\n",
            "Iteration 1019, loss = 0.55556538\n",
            "Iteration 1020, loss = 0.55505347\n",
            "Iteration 1021, loss = 0.55454019\n",
            "Iteration 1022, loss = 0.55402847\n",
            "Iteration 1023, loss = 0.55351397\n",
            "Iteration 1024, loss = 0.55300056\n",
            "Iteration 1025, loss = 0.55248656\n",
            "Iteration 1026, loss = 0.55197203\n",
            "Iteration 1027, loss = 0.55145801\n",
            "Iteration 1028, loss = 0.55094227\n",
            "Iteration 1029, loss = 0.55042659\n",
            "Iteration 1030, loss = 0.54991070\n",
            "Iteration 1031, loss = 0.54939472\n",
            "Iteration 1032, loss = 0.54887801\n",
            "Iteration 1033, loss = 0.54836104\n",
            "Iteration 1034, loss = 0.54784377\n",
            "Iteration 1035, loss = 0.54732591\n",
            "Iteration 1036, loss = 0.54680815\n",
            "Iteration 1037, loss = 0.54628965\n",
            "Iteration 1038, loss = 0.54577076\n",
            "Iteration 1039, loss = 0.54525144\n",
            "Iteration 1040, loss = 0.54473229\n",
            "Iteration 1041, loss = 0.54421227\n",
            "Iteration 1042, loss = 0.54369197\n",
            "Iteration 1043, loss = 0.54317123\n",
            "Iteration 1044, loss = 0.54265023\n",
            "Iteration 1045, loss = 0.54212923\n",
            "Iteration 1046, loss = 0.54160751\n",
            "Iteration 1047, loss = 0.54108535\n",
            "Iteration 1048, loss = 0.54056281\n",
            "Iteration 1049, loss = 0.54004048\n",
            "Iteration 1050, loss = 0.53951722\n",
            "Iteration 1051, loss = 0.53899377\n",
            "Iteration 1052, loss = 0.53846991\n",
            "Iteration 1053, loss = 0.53794568\n",
            "Iteration 1054, loss = 0.53742184\n",
            "Iteration 1055, loss = 0.53689667\n",
            "Iteration 1056, loss = 0.53637152\n",
            "Iteration 1057, loss = 0.53584596\n",
            "Iteration 1058, loss = 0.53532003\n",
            "Iteration 1059, loss = 0.53479415\n",
            "Iteration 1060, loss = 0.53426762\n",
            "Iteration 1061, loss = 0.53374077\n",
            "Iteration 1062, loss = 0.53321349\n",
            "Iteration 1063, loss = 0.53268584\n",
            "Iteration 1064, loss = 0.53215795\n",
            "Iteration 1065, loss = 0.53163003\n",
            "Iteration 1066, loss = 0.53110148\n",
            "Iteration 1067, loss = 0.53057251\n",
            "Iteration 1068, loss = 0.53004316\n",
            "Iteration 1069, loss = 0.52951349\n",
            "Iteration 1070, loss = 0.52898437\n",
            "Iteration 1071, loss = 0.52845359\n",
            "Iteration 1072, loss = 0.52792304\n",
            "Iteration 1073, loss = 0.52739212\n",
            "Iteration 1074, loss = 0.52686083\n",
            "Iteration 1075, loss = 0.52632948\n",
            "Iteration 1076, loss = 0.52579774\n",
            "Iteration 1077, loss = 0.52526557\n",
            "Iteration 1078, loss = 0.52473299\n",
            "Iteration 1079, loss = 0.52420004\n",
            "Iteration 1080, loss = 0.52366677\n",
            "Iteration 1081, loss = 0.52313380\n",
            "Iteration 1082, loss = 0.52259973\n",
            "Iteration 1083, loss = 0.52206562\n",
            "Iteration 1084, loss = 0.52153115\n",
            "Iteration 1085, loss = 0.52099632\n",
            "Iteration 1086, loss = 0.52046148\n",
            "Iteration 1087, loss = 0.51992639\n",
            "Iteration 1088, loss = 0.51939086\n",
            "Iteration 1089, loss = 0.51885494\n",
            "Iteration 1090, loss = 0.51831868\n",
            "Iteration 1091, loss = 0.51778211\n",
            "Iteration 1092, loss = 0.51724595\n",
            "Iteration 1093, loss = 0.51670889\n",
            "Iteration 1094, loss = 0.51617237\n",
            "Iteration 1095, loss = 0.51563593\n",
            "Iteration 1096, loss = 0.51509780\n",
            "Iteration 1097, loss = 0.51456005\n",
            "Iteration 1098, loss = 0.51402266\n",
            "Iteration 1099, loss = 0.51348443\n",
            "Iteration 1100, loss = 0.51294628\n",
            "Iteration 1101, loss = 0.51240742\n",
            "Iteration 1102, loss = 0.51186869\n",
            "Iteration 1103, loss = 0.51132947\n",
            "Iteration 1104, loss = 0.51078991\n",
            "Iteration 1105, loss = 0.51025104\n",
            "Iteration 1106, loss = 0.50971216\n",
            "Iteration 1107, loss = 0.50917130\n",
            "Iteration 1108, loss = 0.50863138\n",
            "Iteration 1109, loss = 0.50809124\n",
            "Iteration 1110, loss = 0.50755126\n",
            "Iteration 1111, loss = 0.50701043\n",
            "Iteration 1112, loss = 0.50646934\n",
            "Iteration 1113, loss = 0.50592783\n",
            "Iteration 1114, loss = 0.50538707\n",
            "Iteration 1115, loss = 0.50484617\n",
            "Iteration 1116, loss = 0.50430439\n",
            "Iteration 1117, loss = 0.50376167\n",
            "Iteration 1118, loss = 0.50322029\n",
            "Iteration 1119, loss = 0.50267797\n",
            "Iteration 1120, loss = 0.50213637\n",
            "Iteration 1121, loss = 0.50159358\n",
            "Iteration 1122, loss = 0.50105058\n",
            "Iteration 1123, loss = 0.50050941\n",
            "Iteration 1124, loss = 0.49996528\n",
            "Iteration 1125, loss = 0.49942284\n",
            "Iteration 1126, loss = 0.49887908\n",
            "Iteration 1127, loss = 0.49833639\n",
            "Iteration 1128, loss = 0.49779281\n",
            "Iteration 1129, loss = 0.49724884\n",
            "Iteration 1130, loss = 0.49670556\n",
            "Iteration 1131, loss = 0.49616135\n",
            "Iteration 1132, loss = 0.49561873\n",
            "Iteration 1133, loss = 0.49507404\n",
            "Iteration 1134, loss = 0.49452985\n",
            "Iteration 1135, loss = 0.49398490\n",
            "Iteration 1136, loss = 0.49344114\n",
            "Iteration 1137, loss = 0.49289737\n",
            "Iteration 1138, loss = 0.49235163\n",
            "Iteration 1139, loss = 0.49180802\n",
            "Iteration 1140, loss = 0.49126307\n",
            "Iteration 1141, loss = 0.49071787\n",
            "Iteration 1142, loss = 0.49017318\n",
            "Iteration 1143, loss = 0.48962763\n",
            "Iteration 1144, loss = 0.48908306\n",
            "Iteration 1145, loss = 0.48853837\n",
            "Iteration 1146, loss = 0.48799286\n",
            "Iteration 1147, loss = 0.48744698\n",
            "Iteration 1148, loss = 0.48690263\n",
            "Iteration 1149, loss = 0.48635685\n",
            "Iteration 1150, loss = 0.48581111\n",
            "Iteration 1151, loss = 0.48526538\n",
            "Iteration 1152, loss = 0.48471998\n",
            "Iteration 1153, loss = 0.48417501\n",
            "Iteration 1154, loss = 0.48362872\n",
            "Iteration 1155, loss = 0.48308299\n",
            "Iteration 1156, loss = 0.48253752\n",
            "Iteration 1157, loss = 0.48199219\n",
            "Iteration 1158, loss = 0.48144617\n",
            "Iteration 1159, loss = 0.48090050\n",
            "Iteration 1160, loss = 0.48035493\n",
            "Iteration 1161, loss = 0.47980890\n",
            "Iteration 1162, loss = 0.47926323\n",
            "Iteration 1163, loss = 0.47871749\n",
            "Iteration 1164, loss = 0.47817209\n",
            "Iteration 1165, loss = 0.47762602\n",
            "Iteration 1166, loss = 0.47708033\n",
            "Iteration 1167, loss = 0.47653490\n",
            "Iteration 1168, loss = 0.47598900\n",
            "Iteration 1169, loss = 0.47544321\n",
            "Iteration 1170, loss = 0.47489790\n",
            "Iteration 1171, loss = 0.47435208\n",
            "Iteration 1172, loss = 0.47380631\n",
            "Iteration 1173, loss = 0.47326123\n",
            "Iteration 1174, loss = 0.47271556\n",
            "Iteration 1175, loss = 0.47216985\n",
            "Iteration 1176, loss = 0.47162529\n",
            "Iteration 1177, loss = 0.47107926\n",
            "Iteration 1178, loss = 0.47053392\n",
            "Iteration 1179, loss = 0.46998931\n",
            "Iteration 1180, loss = 0.46944408\n",
            "Iteration 1181, loss = 0.46889875\n",
            "Iteration 1182, loss = 0.46835358\n",
            "Iteration 1183, loss = 0.46780896\n",
            "Iteration 1184, loss = 0.46726389\n",
            "Iteration 1185, loss = 0.46671901\n",
            "Iteration 1186, loss = 0.46617460\n",
            "Iteration 1187, loss = 0.46562978\n",
            "Iteration 1188, loss = 0.46508494\n",
            "Iteration 1189, loss = 0.46454128\n",
            "Iteration 1190, loss = 0.46399613\n",
            "Iteration 1191, loss = 0.46345172\n",
            "Iteration 1192, loss = 0.46290806\n",
            "Iteration 1193, loss = 0.46236379\n",
            "Iteration 1194, loss = 0.46181944\n",
            "Iteration 1195, loss = 0.46127532\n",
            "Iteration 1196, loss = 0.46073167\n",
            "Iteration 1197, loss = 0.46018764\n",
            "Iteration 1198, loss = 0.45964397\n",
            "Iteration 1199, loss = 0.45910051\n",
            "Iteration 1200, loss = 0.45855680\n",
            "Iteration 1201, loss = 0.45801332\n",
            "Iteration 1202, loss = 0.45747031\n",
            "Iteration 1203, loss = 0.45692690\n",
            "Iteration 1204, loss = 0.45638362\n",
            "Iteration 1205, loss = 0.45584105\n",
            "Iteration 1206, loss = 0.45529797\n",
            "Iteration 1207, loss = 0.45475503\n",
            "Iteration 1208, loss = 0.45421278\n",
            "Iteration 1209, loss = 0.45367004\n",
            "Iteration 1210, loss = 0.45312762\n",
            "Iteration 1211, loss = 0.45258558\n",
            "Iteration 1212, loss = 0.45204322\n",
            "Iteration 1213, loss = 0.45150141\n",
            "Iteration 1214, loss = 0.45095955\n",
            "Iteration 1215, loss = 0.45041761\n",
            "Iteration 1216, loss = 0.44987644\n",
            "Iteration 1217, loss = 0.44933480\n",
            "Iteration 1218, loss = 0.44879330\n",
            "Iteration 1219, loss = 0.44825278\n",
            "Iteration 1220, loss = 0.44771140\n",
            "Iteration 1221, loss = 0.44717044\n",
            "Iteration 1222, loss = 0.44663042\n",
            "Iteration 1223, loss = 0.44608973\n",
            "Iteration 1224, loss = 0.44554903\n",
            "Iteration 1225, loss = 0.44500977\n",
            "Iteration 1226, loss = 0.44446901\n",
            "Iteration 1227, loss = 0.44392990\n",
            "Iteration 1228, loss = 0.44339013\n",
            "Iteration 1229, loss = 0.44285032\n",
            "Iteration 1230, loss = 0.44231161\n",
            "Iteration 1231, loss = 0.44177200\n",
            "Iteration 1232, loss = 0.44123362\n",
            "Iteration 1233, loss = 0.44069478\n",
            "Iteration 1234, loss = 0.44015609\n",
            "Iteration 1235, loss = 0.43961862\n",
            "Iteration 1236, loss = 0.43908010\n",
            "Iteration 1237, loss = 0.43854242\n",
            "Iteration 1238, loss = 0.43800519\n",
            "Iteration 1239, loss = 0.43746758\n",
            "Iteration 1240, loss = 0.43693054\n",
            "Iteration 1241, loss = 0.43639367\n",
            "Iteration 1242, loss = 0.43585688\n",
            "Iteration 1243, loss = 0.43532083\n",
            "Iteration 1244, loss = 0.43478431\n",
            "Iteration 1245, loss = 0.43424892\n",
            "Iteration 1246, loss = 0.43371266\n",
            "Iteration 1247, loss = 0.43317771\n",
            "Iteration 1248, loss = 0.43264224\n",
            "Iteration 1249, loss = 0.43210736\n",
            "Iteration 1250, loss = 0.43157295\n",
            "Iteration 1251, loss = 0.43103820\n",
            "Iteration 1252, loss = 0.43050473\n",
            "Iteration 1253, loss = 0.42997023\n",
            "Iteration 1254, loss = 0.42943743\n",
            "Iteration 1255, loss = 0.42890379\n",
            "Iteration 1256, loss = 0.42837063\n",
            "Iteration 1257, loss = 0.42783799\n",
            "Iteration 1258, loss = 0.42730557\n",
            "Iteration 1259, loss = 0.42677350\n",
            "Iteration 1260, loss = 0.42624161\n",
            "Iteration 1261, loss = 0.42571026\n",
            "Iteration 1262, loss = 0.42517885\n",
            "Iteration 1263, loss = 0.42464824\n",
            "Iteration 1264, loss = 0.42411739\n",
            "Iteration 1265, loss = 0.42358751\n",
            "Iteration 1266, loss = 0.42305719\n",
            "Iteration 1267, loss = 0.42252806\n",
            "Iteration 1268, loss = 0.42199832\n",
            "Iteration 1269, loss = 0.42146990\n",
            "Iteration 1270, loss = 0.42094084\n",
            "Iteration 1271, loss = 0.42041303\n",
            "Iteration 1272, loss = 0.41988483\n",
            "Iteration 1273, loss = 0.41935756\n",
            "Iteration 1274, loss = 0.41883019\n",
            "Iteration 1275, loss = 0.41830347\n",
            "Iteration 1276, loss = 0.41777707\n",
            "Iteration 1277, loss = 0.41725087\n",
            "Iteration 1278, loss = 0.41672535\n",
            "Iteration 1279, loss = 0.41619980\n",
            "Iteration 1280, loss = 0.41567511\n",
            "Iteration 1281, loss = 0.41515025\n",
            "Iteration 1282, loss = 0.41462643\n",
            "Iteration 1283, loss = 0.41410222\n",
            "Iteration 1284, loss = 0.41357935\n",
            "Iteration 1285, loss = 0.41305591\n",
            "Iteration 1286, loss = 0.41253395\n",
            "Iteration 1287, loss = 0.41201126\n",
            "Iteration 1288, loss = 0.41148992\n",
            "Iteration 1289, loss = 0.41096822\n",
            "Iteration 1290, loss = 0.41044745\n",
            "Iteration 1291, loss = 0.40992679\n",
            "Iteration 1292, loss = 0.40940682\n",
            "Iteration 1293, loss = 0.40888723\n",
            "Iteration 1294, loss = 0.40836772\n",
            "Iteration 1295, loss = 0.40784966\n",
            "Iteration 1296, loss = 0.40733067\n",
            "Iteration 1297, loss = 0.40681354\n",
            "Iteration 1298, loss = 0.40629588\n",
            "Iteration 1299, loss = 0.40577852\n",
            "Iteration 1300, loss = 0.40526423\n",
            "Iteration 1301, loss = 0.40474902\n",
            "Iteration 1302, loss = 0.40423516\n",
            "Iteration 1303, loss = 0.40372171\n",
            "Iteration 1304, loss = 0.40320874\n",
            "Iteration 1305, loss = 0.40269648\n",
            "Iteration 1306, loss = 0.40218535\n",
            "Iteration 1307, loss = 0.40167364\n",
            "Iteration 1308, loss = 0.40116306\n",
            "Iteration 1309, loss = 0.40065363\n",
            "Iteration 1310, loss = 0.40014379\n",
            "Iteration 1311, loss = 0.39963499\n",
            "Iteration 1312, loss = 0.39912738\n",
            "Iteration 1313, loss = 0.39861984\n",
            "Iteration 1314, loss = 0.39811320\n",
            "Iteration 1315, loss = 0.39760702\n",
            "Iteration 1316, loss = 0.39710148\n",
            "Iteration 1317, loss = 0.39659673\n",
            "Iteration 1318, loss = 0.39609230\n",
            "Iteration 1319, loss = 0.39558844\n",
            "Iteration 1320, loss = 0.39508567\n",
            "Iteration 1321, loss = 0.39458359\n",
            "Iteration 1322, loss = 0.39408123\n",
            "Iteration 1323, loss = 0.39358032\n",
            "Iteration 1324, loss = 0.39307974\n",
            "Iteration 1325, loss = 0.39257984\n",
            "Iteration 1326, loss = 0.39208050\n",
            "Iteration 1327, loss = 0.39158175\n",
            "Iteration 1328, loss = 0.39108381\n",
            "Iteration 1329, loss = 0.39058694\n",
            "Iteration 1330, loss = 0.39008967\n",
            "Iteration 1331, loss = 0.38959402\n",
            "Iteration 1332, loss = 0.38909836\n",
            "Iteration 1333, loss = 0.38860349\n",
            "Iteration 1334, loss = 0.38810922\n",
            "Iteration 1335, loss = 0.38761556\n",
            "Iteration 1336, loss = 0.38712273\n",
            "Iteration 1337, loss = 0.38663055\n",
            "Iteration 1338, loss = 0.38613888\n",
            "Iteration 1339, loss = 0.38564781\n",
            "Iteration 1340, loss = 0.38515735\n",
            "Iteration 1341, loss = 0.38466767\n",
            "Iteration 1342, loss = 0.38417875\n",
            "Iteration 1343, loss = 0.38369029\n",
            "Iteration 1344, loss = 0.38320244\n",
            "Iteration 1345, loss = 0.38271521\n",
            "Iteration 1346, loss = 0.38222878\n",
            "Iteration 1347, loss = 0.38174309\n",
            "Iteration 1348, loss = 0.38125788\n",
            "Iteration 1349, loss = 0.38077328\n",
            "Iteration 1350, loss = 0.38028933\n",
            "Iteration 1351, loss = 0.37980603\n",
            "Iteration 1352, loss = 0.37932370\n",
            "Iteration 1353, loss = 0.37884166\n",
            "Iteration 1354, loss = 0.37836040\n",
            "Iteration 1355, loss = 0.37787976\n",
            "Iteration 1356, loss = 0.37739975\n",
            "Iteration 1357, loss = 0.37692040\n",
            "Iteration 1358, loss = 0.37644171\n",
            "Iteration 1359, loss = 0.37596397\n",
            "Iteration 1360, loss = 0.37548656\n",
            "Iteration 1361, loss = 0.37500992\n",
            "Iteration 1362, loss = 0.37453392\n",
            "Iteration 1363, loss = 0.37405857\n",
            "Iteration 1364, loss = 0.37358388\n",
            "Iteration 1365, loss = 0.37310987\n",
            "Iteration 1366, loss = 0.37263654\n",
            "Iteration 1367, loss = 0.37216390\n",
            "Iteration 1368, loss = 0.37169206\n",
            "Iteration 1369, loss = 0.37122083\n",
            "Iteration 1370, loss = 0.37075022\n",
            "Iteration 1371, loss = 0.37028027\n",
            "Iteration 1372, loss = 0.36981099\n",
            "Iteration 1373, loss = 0.36934239\n",
            "Iteration 1374, loss = 0.36887449\n",
            "Iteration 1375, loss = 0.36840728\n",
            "Iteration 1376, loss = 0.36794077\n",
            "Iteration 1377, loss = 0.36747495\n",
            "Iteration 1378, loss = 0.36700983\n",
            "Iteration 1379, loss = 0.36654539\n",
            "Iteration 1380, loss = 0.36608166\n",
            "Iteration 1381, loss = 0.36561861\n",
            "Iteration 1382, loss = 0.36515626\n",
            "Iteration 1383, loss = 0.36469460\n",
            "Iteration 1384, loss = 0.36423364\n",
            "Iteration 1385, loss = 0.36377338\n",
            "Iteration 1386, loss = 0.36331384\n",
            "Iteration 1387, loss = 0.36285498\n",
            "Iteration 1388, loss = 0.36239684\n",
            "Iteration 1389, loss = 0.36193940\n",
            "Iteration 1390, loss = 0.36148267\n",
            "Iteration 1391, loss = 0.36102665\n",
            "Iteration 1392, loss = 0.36057135\n",
            "Iteration 1393, loss = 0.36011675\n",
            "Iteration 1394, loss = 0.35966286\n",
            "Iteration 1395, loss = 0.35920970\n",
            "Iteration 1396, loss = 0.35875725\n",
            "Iteration 1397, loss = 0.35830551\n",
            "Iteration 1398, loss = 0.35785450\n",
            "Iteration 1399, loss = 0.35740420\n",
            "Iteration 1400, loss = 0.35695463\n",
            "Iteration 1401, loss = 0.35650578\n",
            "Iteration 1402, loss = 0.35605766\n",
            "Iteration 1403, loss = 0.35561026\n",
            "Iteration 1404, loss = 0.35516359\n",
            "Iteration 1405, loss = 0.35471765\n",
            "Iteration 1406, loss = 0.35427244\n",
            "Iteration 1407, loss = 0.35382796\n",
            "Iteration 1408, loss = 0.35338422\n",
            "Iteration 1409, loss = 0.35294120\n",
            "Iteration 1410, loss = 0.35249893\n",
            "Iteration 1411, loss = 0.35205738\n",
            "Iteration 1412, loss = 0.35161658\n",
            "Iteration 1413, loss = 0.35117652\n",
            "Iteration 1414, loss = 0.35073719\n",
            "Iteration 1415, loss = 0.35029861\n",
            "Iteration 1416, loss = 0.34986077\n",
            "Iteration 1417, loss = 0.34942367\n",
            "Iteration 1418, loss = 0.34898732\n",
            "Iteration 1419, loss = 0.34855171\n",
            "Iteration 1420, loss = 0.34811685\n",
            "Iteration 1421, loss = 0.34768274\n",
            "Iteration 1422, loss = 0.34724939\n",
            "Iteration 1423, loss = 0.34681677\n",
            "Iteration 1424, loss = 0.34638491\n",
            "Iteration 1425, loss = 0.34595381\n",
            "Iteration 1426, loss = 0.34552346\n",
            "Iteration 1427, loss = 0.34509386\n",
            "Iteration 1428, loss = 0.34466502\n",
            "Iteration 1429, loss = 0.34423693\n",
            "Iteration 1430, loss = 0.34380960\n",
            "Iteration 1431, loss = 0.34338303\n",
            "Iteration 1432, loss = 0.34295723\n",
            "Iteration 1433, loss = 0.34253218\n",
            "Iteration 1434, loss = 0.34210789\n",
            "Iteration 1435, loss = 0.34168437\n",
            "Iteration 1436, loss = 0.34126161\n",
            "Iteration 1437, loss = 0.34083961\n",
            "Iteration 1438, loss = 0.34041838\n",
            "Iteration 1439, loss = 0.33999791\n",
            "Iteration 1440, loss = 0.33957821\n",
            "Iteration 1441, loss = 0.33915928\n",
            "Iteration 1442, loss = 0.33874112\n",
            "Iteration 1443, loss = 0.33832372\n",
            "Iteration 1444, loss = 0.33790710\n",
            "Iteration 1445, loss = 0.33749124\n",
            "Iteration 1446, loss = 0.33707616\n",
            "Iteration 1447, loss = 0.33666185\n",
            "Iteration 1448, loss = 0.33624831\n",
            "Iteration 1449, loss = 0.33583555\n",
            "Iteration 1450, loss = 0.33542356\n",
            "Iteration 1451, loss = 0.33501235\n",
            "Iteration 1452, loss = 0.33460217\n",
            "Iteration 1453, loss = 0.33419352\n",
            "Iteration 1454, loss = 0.33378564\n",
            "Iteration 1455, loss = 0.33337868\n",
            "Iteration 1456, loss = 0.33297265\n",
            "Iteration 1457, loss = 0.33256746\n",
            "Iteration 1458, loss = 0.33216328\n",
            "Iteration 1459, loss = 0.33175943\n",
            "Iteration 1460, loss = 0.33135662\n",
            "Iteration 1461, loss = 0.33095465\n",
            "Iteration 1462, loss = 0.33055352\n",
            "Iteration 1463, loss = 0.33015324\n",
            "Iteration 1464, loss = 0.32975383\n",
            "Iteration 1465, loss = 0.32935528\n",
            "Iteration 1466, loss = 0.32895760\n",
            "Iteration 1467, loss = 0.32856123\n",
            "Iteration 1468, loss = 0.32816505\n",
            "Iteration 1469, loss = 0.32777002\n",
            "Iteration 1470, loss = 0.32737580\n",
            "Iteration 1471, loss = 0.32698242\n",
            "Iteration 1472, loss = 0.32658990\n",
            "Iteration 1473, loss = 0.32619826\n",
            "Iteration 1474, loss = 0.32580744\n",
            "Iteration 1475, loss = 0.32541741\n",
            "Iteration 1476, loss = 0.32502818\n",
            "Iteration 1477, loss = 0.32463977\n",
            "Iteration 1478, loss = 0.32425219\n",
            "Iteration 1479, loss = 0.32386542\n",
            "Iteration 1480, loss = 0.32347948\n",
            "Iteration 1481, loss = 0.32309435\n",
            "Iteration 1482, loss = 0.32271002\n",
            "Iteration 1483, loss = 0.32232649\n",
            "Iteration 1484, loss = 0.32194376\n",
            "Iteration 1485, loss = 0.32156186\n",
            "Iteration 1486, loss = 0.32118075\n",
            "Iteration 1487, loss = 0.32080044\n",
            "Iteration 1488, loss = 0.32042091\n",
            "Iteration 1489, loss = 0.32004217\n",
            "Iteration 1490, loss = 0.31966422\n",
            "Iteration 1491, loss = 0.31928706\n",
            "Iteration 1492, loss = 0.31891068\n",
            "Iteration 1493, loss = 0.31853509\n",
            "Iteration 1494, loss = 0.31816028\n",
            "Iteration 1495, loss = 0.31778625\n",
            "Iteration 1496, loss = 0.31741300\n",
            "Iteration 1497, loss = 0.31704053\n",
            "Iteration 1498, loss = 0.31666885\n",
            "Iteration 1499, loss = 0.31629794\n",
            "Iteration 1500, loss = 0.31592780\n",
            "Iteration 1501, loss = 0.31555845\n",
            "Iteration 1502, loss = 0.31518987\n",
            "Iteration 1503, loss = 0.31482207\n",
            "Iteration 1504, loss = 0.31445504\n",
            "Iteration 1505, loss = 0.31408879\n",
            "Iteration 1506, loss = 0.31372331\n",
            "Iteration 1507, loss = 0.31335860\n",
            "Iteration 1508, loss = 0.31299467\n",
            "Iteration 1509, loss = 0.31263151\n",
            "Iteration 1510, loss = 0.31226912\n",
            "Iteration 1511, loss = 0.31190750\n",
            "Iteration 1512, loss = 0.31154665\n",
            "Iteration 1513, loss = 0.31118657\n",
            "Iteration 1514, loss = 0.31082726\n",
            "Iteration 1515, loss = 0.31046871\n",
            "Iteration 1516, loss = 0.31011093\n",
            "Iteration 1517, loss = 0.30975391\n",
            "Iteration 1518, loss = 0.30939766\n",
            "Iteration 1519, loss = 0.30904217\n",
            "Iteration 1520, loss = 0.30868745\n",
            "Iteration 1521, loss = 0.30833348\n",
            "Iteration 1522, loss = 0.30798028\n",
            "Iteration 1523, loss = 0.30762784\n",
            "Iteration 1524, loss = 0.30727615\n",
            "Iteration 1525, loss = 0.30692523\n",
            "Iteration 1526, loss = 0.30657507\n",
            "Iteration 1527, loss = 0.30622566\n",
            "Iteration 1528, loss = 0.30587701\n",
            "Iteration 1529, loss = 0.30552911\n",
            "Iteration 1530, loss = 0.30518197\n",
            "Iteration 1531, loss = 0.30483558\n",
            "Iteration 1532, loss = 0.30448995\n",
            "Iteration 1533, loss = 0.30414507\n",
            "Iteration 1534, loss = 0.30380094\n",
            "Iteration 1535, loss = 0.30345756\n",
            "Iteration 1536, loss = 0.30311494\n",
            "Iteration 1537, loss = 0.30277306\n",
            "Iteration 1538, loss = 0.30243193\n",
            "Iteration 1539, loss = 0.30209155\n",
            "Iteration 1540, loss = 0.30175192\n",
            "Iteration 1541, loss = 0.30141303\n",
            "Iteration 1542, loss = 0.30107489\n",
            "Iteration 1543, loss = 0.30073749\n",
            "Iteration 1544, loss = 0.30040084\n",
            "Iteration 1545, loss = 0.30006493\n",
            "Iteration 1546, loss = 0.29972977\n",
            "Iteration 1547, loss = 0.29939534\n",
            "Iteration 1548, loss = 0.29906166\n",
            "Iteration 1549, loss = 0.29872872\n",
            "Iteration 1550, loss = 0.29839651\n",
            "Iteration 1551, loss = 0.29806505\n",
            "Iteration 1552, loss = 0.29773432\n",
            "Iteration 1553, loss = 0.29740433\n",
            "Iteration 1554, loss = 0.29707508\n",
            "Iteration 1555, loss = 0.29674656\n",
            "Iteration 1556, loss = 0.29641877\n",
            "Iteration 1557, loss = 0.29609172\n",
            "Iteration 1558, loss = 0.29576541\n",
            "Iteration 1559, loss = 0.29543982\n",
            "Iteration 1560, loss = 0.29511496\n",
            "Iteration 1561, loss = 0.29479084\n",
            "Iteration 1562, loss = 0.29446744\n",
            "Iteration 1563, loss = 0.29414477\n",
            "Iteration 1564, loss = 0.29382283\n",
            "Iteration 1565, loss = 0.29350162\n",
            "Iteration 1566, loss = 0.29318113\n",
            "Iteration 1567, loss = 0.29286137\n",
            "Iteration 1568, loss = 0.29254233\n",
            "Iteration 1569, loss = 0.29222401\n",
            "Iteration 1570, loss = 0.29190642\n",
            "Iteration 1571, loss = 0.29158954\n",
            "Iteration 1572, loss = 0.29127339\n",
            "Iteration 1573, loss = 0.29095796\n",
            "Iteration 1574, loss = 0.29064324\n",
            "Iteration 1575, loss = 0.29032925\n",
            "Iteration 1576, loss = 0.29001596\n",
            "Iteration 1577, loss = 0.28970340\n",
            "Iteration 1578, loss = 0.28939155\n",
            "Iteration 1579, loss = 0.28908041\n",
            "Iteration 1580, loss = 0.28876999\n",
            "Iteration 1581, loss = 0.28846028\n",
            "Iteration 1582, loss = 0.28815127\n",
            "Iteration 1583, loss = 0.28784298\n",
            "Iteration 1584, loss = 0.28753540\n",
            "Iteration 1585, loss = 0.28722852\n",
            "Iteration 1586, loss = 0.28692235\n",
            "Iteration 1587, loss = 0.28661689\n",
            "Iteration 1588, loss = 0.28631213\n",
            "Iteration 1589, loss = 0.28600808\n",
            "Iteration 1590, loss = 0.28570472\n",
            "Iteration 1591, loss = 0.28540208\n",
            "Iteration 1592, loss = 0.28510013\n",
            "Iteration 1593, loss = 0.28479888\n",
            "Iteration 1594, loss = 0.28449833\n",
            "Iteration 1595, loss = 0.28419847\n",
            "Iteration 1596, loss = 0.28389932\n",
            "Iteration 1597, loss = 0.28360092\n",
            "Iteration 1598, loss = 0.28330326\n",
            "Iteration 1599, loss = 0.28300630\n",
            "Iteration 1600, loss = 0.28271005\n",
            "Iteration 1601, loss = 0.28241451\n",
            "Iteration 1602, loss = 0.28211968\n",
            "Iteration 1603, loss = 0.28182555\n",
            "Iteration 1604, loss = 0.28153212\n",
            "Iteration 1605, loss = 0.28123938\n",
            "Iteration 1606, loss = 0.28094733\n",
            "Iteration 1607, loss = 0.28065597\n",
            "Iteration 1608, loss = 0.28036530\n",
            "Iteration 1609, loss = 0.28007530\n",
            "Iteration 1610, loss = 0.27978599\n",
            "Iteration 1611, loss = 0.27949737\n",
            "Iteration 1612, loss = 0.27920942\n",
            "Iteration 1613, loss = 0.27892215\n",
            "Iteration 1614, loss = 0.27863556\n",
            "Iteration 1615, loss = 0.27834966\n",
            "Iteration 1616, loss = 0.27806442\n",
            "Iteration 1617, loss = 0.27777987\n",
            "Iteration 1618, loss = 0.27749599\n",
            "Iteration 1619, loss = 0.27721278\n",
            "Iteration 1620, loss = 0.27693024\n",
            "Iteration 1621, loss = 0.27664837\n",
            "Iteration 1622, loss = 0.27636717\n",
            "Iteration 1623, loss = 0.27608664\n",
            "Iteration 1624, loss = 0.27580677\n",
            "Iteration 1625, loss = 0.27552757\n",
            "Iteration 1626, loss = 0.27524903\n",
            "Iteration 1627, loss = 0.27497115\n",
            "Iteration 1628, loss = 0.27469394\n",
            "Iteration 1629, loss = 0.27441738\n",
            "Iteration 1630, loss = 0.27414148\n",
            "Iteration 1631, loss = 0.27386624\n",
            "Iteration 1632, loss = 0.27359166\n",
            "Iteration 1633, loss = 0.27331773\n",
            "Iteration 1634, loss = 0.27304446\n",
            "Iteration 1635, loss = 0.27277184\n",
            "Iteration 1636, loss = 0.27249987\n",
            "Iteration 1637, loss = 0.27222855\n",
            "Iteration 1638, loss = 0.27195788\n",
            "Iteration 1639, loss = 0.27168785\n",
            "Iteration 1640, loss = 0.27141847\n",
            "Iteration 1641, loss = 0.27114974\n",
            "Iteration 1642, loss = 0.27088165\n",
            "Iteration 1643, loss = 0.27061421\n",
            "Iteration 1644, loss = 0.27034740\n",
            "Iteration 1645, loss = 0.27008123\n",
            "Iteration 1646, loss = 0.26981571\n",
            "Iteration 1647, loss = 0.26955082\n",
            "Iteration 1648, loss = 0.26928657\n",
            "Iteration 1649, loss = 0.26902295\n",
            "Iteration 1650, loss = 0.26875997\n",
            "Iteration 1651, loss = 0.26849761\n",
            "Iteration 1652, loss = 0.26823590\n",
            "Iteration 1653, loss = 0.26797480\n",
            "Iteration 1654, loss = 0.26771434\n",
            "Iteration 1655, loss = 0.26745451\n",
            "Iteration 1656, loss = 0.26719530\n",
            "Iteration 1657, loss = 0.26693672\n",
            "Iteration 1658, loss = 0.26667876\n",
            "Iteration 1659, loss = 0.26642142\n",
            "Iteration 1660, loss = 0.26616471\n",
            "Iteration 1661, loss = 0.26590861\n",
            "Iteration 1662, loss = 0.26565313\n",
            "Iteration 1663, loss = 0.26539827\n",
            "Iteration 1664, loss = 0.26514403\n",
            "Iteration 1665, loss = 0.26489040\n",
            "Iteration 1666, loss = 0.26463738\n",
            "Iteration 1667, loss = 0.26438498\n",
            "Iteration 1668, loss = 0.26413319\n",
            "Iteration 1669, loss = 0.26388200\n",
            "Iteration 1670, loss = 0.26363143\n",
            "Iteration 1671, loss = 0.26338146\n",
            "Iteration 1672, loss = 0.26313209\n",
            "Iteration 1673, loss = 0.26288333\n",
            "Iteration 1674, loss = 0.26263518\n",
            "Iteration 1675, loss = 0.26238762\n",
            "Iteration 1676, loss = 0.26214067\n",
            "Iteration 1677, loss = 0.26189432\n",
            "Iteration 1678, loss = 0.26164856\n",
            "Iteration 1679, loss = 0.26140340\n",
            "Iteration 1680, loss = 0.26115884\n",
            "Iteration 1681, loss = 0.26091486\n",
            "Iteration 1682, loss = 0.26067149\n",
            "Iteration 1683, loss = 0.26042870\n",
            "Iteration 1684, loss = 0.26018650\n",
            "Iteration 1685, loss = 0.25994489\n",
            "Iteration 1686, loss = 0.25970387\n",
            "Iteration 1687, loss = 0.25946344\n",
            "Iteration 1688, loss = 0.25922359\n",
            "Iteration 1689, loss = 0.25898432\n",
            "Iteration 1690, loss = 0.25874564\n",
            "Iteration 1691, loss = 0.25850753\n",
            "Iteration 1692, loss = 0.25827001\n",
            "Iteration 1693, loss = 0.25803306\n",
            "Iteration 1694, loss = 0.25779669\n",
            "Iteration 1695, loss = 0.25756090\n",
            "Iteration 1696, loss = 0.25732568\n",
            "Iteration 1697, loss = 0.25709104\n",
            "Iteration 1698, loss = 0.25685696\n",
            "Iteration 1699, loss = 0.25662346\n",
            "Iteration 1700, loss = 0.25639053\n",
            "Iteration 1701, loss = 0.25615816\n",
            "Iteration 1702, loss = 0.25592636\n",
            "Iteration 1703, loss = 0.25569512\n",
            "Iteration 1704, loss = 0.25546445\n",
            "Iteration 1705, loss = 0.25523434\n",
            "Iteration 1706, loss = 0.25500480\n",
            "Iteration 1707, loss = 0.25477581\n",
            "Iteration 1708, loss = 0.25454738\n",
            "Iteration 1709, loss = 0.25431951\n",
            "Iteration 1710, loss = 0.25409219\n",
            "Iteration 1711, loss = 0.25386543\n",
            "Iteration 1712, loss = 0.25363923\n",
            "Iteration 1713, loss = 0.25341357\n",
            "Iteration 1714, loss = 0.25318847\n",
            "Iteration 1715, loss = 0.25296391\n",
            "Iteration 1716, loss = 0.25273990\n",
            "Iteration 1717, loss = 0.25251645\n",
            "Iteration 1718, loss = 0.25229353\n",
            "Iteration 1719, loss = 0.25207116\n",
            "Iteration 1720, loss = 0.25184933\n",
            "Iteration 1721, loss = 0.25162805\n",
            "Iteration 1722, loss = 0.25140730\n",
            "Iteration 1723, loss = 0.25118710\n",
            "Iteration 1724, loss = 0.25096743\n",
            "Iteration 1725, loss = 0.25074830\n",
            "Iteration 1726, loss = 0.25052970\n",
            "Iteration 1727, loss = 0.25031164\n",
            "Iteration 1728, loss = 0.25009411\n",
            "Iteration 1729, loss = 0.24987711\n",
            "Iteration 1730, loss = 0.24966064\n",
            "Iteration 1731, loss = 0.24944470\n",
            "Iteration 1732, loss = 0.24922929\n",
            "Iteration 1733, loss = 0.24901440\n",
            "Iteration 1734, loss = 0.24880004\n",
            "Iteration 1735, loss = 0.24858620\n",
            "Iteration 1736, loss = 0.24837288\n",
            "Iteration 1737, loss = 0.24816009\n",
            "Iteration 1738, loss = 0.24794781\n",
            "Iteration 1739, loss = 0.24773605\n",
            "Iteration 1740, loss = 0.24752481\n",
            "Iteration 1741, loss = 0.24731409\n",
            "Iteration 1742, loss = 0.24710388\n",
            "Iteration 1743, loss = 0.24689418\n",
            "Iteration 1744, loss = 0.24668499\n",
            "Iteration 1745, loss = 0.24647632\n",
            "Iteration 1746, loss = 0.24626816\n",
            "Iteration 1747, loss = 0.24606050\n",
            "Iteration 1748, loss = 0.24585335\n",
            "Iteration 1749, loss = 0.24564670\n",
            "Iteration 1750, loss = 0.24544056\n",
            "Iteration 1751, loss = 0.24523493\n",
            "Iteration 1752, loss = 0.24502979\n",
            "Iteration 1753, loss = 0.24482516\n",
            "Iteration 1754, loss = 0.24462102\n",
            "Iteration 1755, loss = 0.24441738\n",
            "Iteration 1756, loss = 0.24421424\n",
            "Iteration 1757, loss = 0.24401159\n",
            "Iteration 1758, loss = 0.24380944\n",
            "Iteration 1759, loss = 0.24360778\n",
            "Iteration 1760, loss = 0.24340662\n",
            "Iteration 1761, loss = 0.24320655\n",
            "Iteration 1762, loss = 0.24300705\n",
            "Iteration 1763, loss = 0.24280808\n",
            "Iteration 1764, loss = 0.24260973\n",
            "Iteration 1765, loss = 0.24241197\n",
            "Iteration 1766, loss = 0.24221473\n",
            "Iteration 1767, loss = 0.24201795\n",
            "Iteration 1768, loss = 0.24182163\n",
            "Iteration 1769, loss = 0.24162578\n",
            "Iteration 1770, loss = 0.24143043\n",
            "Iteration 1771, loss = 0.24123561\n",
            "Iteration 1772, loss = 0.24104132\n",
            "Iteration 1773, loss = 0.24084758\n",
            "Iteration 1774, loss = 0.24065436\n",
            "Iteration 1775, loss = 0.24046166\n",
            "Iteration 1776, loss = 0.24026947\n",
            "Iteration 1777, loss = 0.24007780\n",
            "Iteration 1778, loss = 0.23988664\n",
            "Iteration 1779, loss = 0.23969599\n",
            "Iteration 1780, loss = 0.23950584\n",
            "Iteration 1781, loss = 0.23931620\n",
            "Iteration 1782, loss = 0.23912705\n",
            "Iteration 1783, loss = 0.23893839\n",
            "Iteration 1784, loss = 0.23875021\n",
            "Iteration 1785, loss = 0.23856252\n",
            "Iteration 1786, loss = 0.23837529\n",
            "Iteration 1787, loss = 0.23818854\n",
            "Iteration 1788, loss = 0.23800226\n",
            "Iteration 1789, loss = 0.23781643\n",
            "Iteration 1790, loss = 0.23763107\n",
            "Iteration 1791, loss = 0.23744616\n",
            "Iteration 1792, loss = 0.23726171\n",
            "Iteration 1793, loss = 0.23707770\n",
            "Iteration 1794, loss = 0.23689414\n",
            "Iteration 1795, loss = 0.23671103\n",
            "Iteration 1796, loss = 0.23652836\n",
            "Iteration 1797, loss = 0.23634613\n",
            "Iteration 1798, loss = 0.23616434\n",
            "Iteration 1799, loss = 0.23598299\n",
            "Iteration 1800, loss = 0.23580207\n",
            "Iteration 1801, loss = 0.23562159\n",
            "Iteration 1802, loss = 0.23544154\n",
            "Iteration 1803, loss = 0.23526192\n",
            "Iteration 1804, loss = 0.23508273\n",
            "Iteration 1805, loss = 0.23490397\n",
            "Iteration 1806, loss = 0.23472564\n",
            "Iteration 1807, loss = 0.23454773\n",
            "Iteration 1808, loss = 0.23437026\n",
            "Iteration 1809, loss = 0.23419320\n",
            "Iteration 1810, loss = 0.23401657\n",
            "Iteration 1811, loss = 0.23384036\n",
            "Iteration 1812, loss = 0.23366457\n",
            "Iteration 1813, loss = 0.23348920\n",
            "Iteration 1814, loss = 0.23331425\n",
            "Iteration 1815, loss = 0.23313971\n",
            "Iteration 1816, loss = 0.23296560\n",
            "Iteration 1817, loss = 0.23279189\n",
            "Iteration 1818, loss = 0.23261861\n",
            "Iteration 1819, loss = 0.23244573\n",
            "Iteration 1820, loss = 0.23227326\n",
            "Iteration 1821, loss = 0.23210121\n",
            "Iteration 1822, loss = 0.23192956\n",
            "Iteration 1823, loss = 0.23175832\n",
            "Iteration 1824, loss = 0.23158749\n",
            "Iteration 1825, loss = 0.23141707\n",
            "Iteration 1826, loss = 0.23124705\n",
            "Iteration 1827, loss = 0.23107743\n",
            "Iteration 1828, loss = 0.23090821\n",
            "Iteration 1829, loss = 0.23073939\n",
            "Iteration 1830, loss = 0.23057098\n",
            "Iteration 1831, loss = 0.23040296\n",
            "Iteration 1832, loss = 0.23023534\n",
            "Iteration 1833, loss = 0.23006811\n",
            "Iteration 1834, loss = 0.22990128\n",
            "Iteration 1835, loss = 0.22973485\n",
            "Iteration 1836, loss = 0.22956880\n",
            "Iteration 1837, loss = 0.22940315\n",
            "Iteration 1838, loss = 0.22923789\n",
            "Iteration 1839, loss = 0.22907302\n",
            "Iteration 1840, loss = 0.22890853\n",
            "Iteration 1841, loss = 0.22874444\n",
            "Iteration 1842, loss = 0.22858073\n",
            "Iteration 1843, loss = 0.22841740\n",
            "Iteration 1844, loss = 0.22825446\n",
            "Iteration 1845, loss = 0.22809190\n",
            "Iteration 1846, loss = 0.22792973\n",
            "Iteration 1847, loss = 0.22776793\n",
            "Iteration 1848, loss = 0.22760651\n",
            "Iteration 1849, loss = 0.22744547\n",
            "Iteration 1850, loss = 0.22728481\n",
            "Iteration 1851, loss = 0.22712453\n",
            "Iteration 1852, loss = 0.22696462\n",
            "Iteration 1853, loss = 0.22680509\n",
            "Iteration 1854, loss = 0.22664593\n",
            "Iteration 1855, loss = 0.22648714\n",
            "Iteration 1856, loss = 0.22632872\n",
            "Iteration 1857, loss = 0.22617067\n",
            "Iteration 1858, loss = 0.22601299\n",
            "Iteration 1859, loss = 0.22585568\n",
            "Iteration 1860, loss = 0.22569874\n",
            "Iteration 1861, loss = 0.22554217\n",
            "Iteration 1862, loss = 0.22538595\n",
            "Iteration 1863, loss = 0.22523011\n",
            "Iteration 1864, loss = 0.22507462\n",
            "Iteration 1865, loss = 0.22491950\n",
            "Iteration 1866, loss = 0.22476474\n",
            "Iteration 1867, loss = 0.22461034\n",
            "Iteration 1868, loss = 0.22445629\n",
            "Iteration 1869, loss = 0.22430261\n",
            "Iteration 1870, loss = 0.22414928\n",
            "Iteration 1871, loss = 0.22399631\n",
            "Iteration 1872, loss = 0.22384370\n",
            "Iteration 1873, loss = 0.22369144\n",
            "Iteration 1874, loss = 0.22353953\n",
            "Iteration 1875, loss = 0.22338797\n",
            "Iteration 1876, loss = 0.22323677\n",
            "Iteration 1877, loss = 0.22308591\n",
            "Iteration 1878, loss = 0.22293541\n",
            "Iteration 1879, loss = 0.22278525\n",
            "Iteration 1880, loss = 0.22263544\n",
            "Iteration 1881, loss = 0.22248597\n",
            "Iteration 1882, loss = 0.22233686\n",
            "Iteration 1883, loss = 0.22218808\n",
            "Iteration 1884, loss = 0.22203965\n",
            "Iteration 1885, loss = 0.22189157\n",
            "Iteration 1886, loss = 0.22174382\n",
            "Iteration 1887, loss = 0.22159642\n",
            "Iteration 1888, loss = 0.22144935\n",
            "Iteration 1889, loss = 0.22130262\n",
            "Iteration 1890, loss = 0.22115624\n",
            "Iteration 1891, loss = 0.22101018\n",
            "Iteration 1892, loss = 0.22086447\n",
            "Iteration 1893, loss = 0.22071909\n",
            "Iteration 1894, loss = 0.22057404\n",
            "Iteration 1895, loss = 0.22042933\n",
            "Iteration 1896, loss = 0.22028495\n",
            "Iteration 1897, loss = 0.22014090\n",
            "Iteration 1898, loss = 0.21999719\n",
            "Iteration 1899, loss = 0.21985380\n",
            "Iteration 1900, loss = 0.21971074\n",
            "Iteration 1901, loss = 0.21956801\n",
            "Iteration 1902, loss = 0.21942560\n",
            "Iteration 1903, loss = 0.21928352\n",
            "Iteration 1904, loss = 0.21914177\n",
            "Iteration 1905, loss = 0.21900034\n",
            "Iteration 1906, loss = 0.21885924\n",
            "Iteration 1907, loss = 0.21871845\n",
            "Iteration 1908, loss = 0.21857799\n",
            "Iteration 1909, loss = 0.21843785\n",
            "Iteration 1910, loss = 0.21829803\n",
            "Iteration 1911, loss = 0.21815853\n",
            "Iteration 1912, loss = 0.21801935\n",
            "Iteration 1913, loss = 0.21788048\n",
            "Iteration 1914, loss = 0.21774193\n",
            "Iteration 1915, loss = 0.21760370\n",
            "Iteration 1916, loss = 0.21746578\n",
            "Iteration 1917, loss = 0.21732817\n",
            "Iteration 1918, loss = 0.21719088\n",
            "Iteration 1919, loss = 0.21705390\n",
            "Iteration 1920, loss = 0.21691723\n",
            "Iteration 1921, loss = 0.21678087\n",
            "Iteration 1922, loss = 0.21664482\n",
            "Iteration 1923, loss = 0.21650908\n",
            "Iteration 1924, loss = 0.21637365\n",
            "Iteration 1925, loss = 0.21623852\n",
            "Iteration 1926, loss = 0.21610370\n",
            "Iteration 1927, loss = 0.21596919\n",
            "Iteration 1928, loss = 0.21583498\n",
            "Iteration 1929, loss = 0.21570107\n",
            "Iteration 1930, loss = 0.21556747\n",
            "Iteration 1931, loss = 0.21543417\n",
            "Iteration 1932, loss = 0.21530117\n",
            "Iteration 1933, loss = 0.21516847\n",
            "Iteration 1934, loss = 0.21503607\n",
            "Iteration 1935, loss = 0.21490397\n",
            "Iteration 1936, loss = 0.21477217\n",
            "Iteration 1937, loss = 0.21464066\n",
            "Iteration 1938, loss = 0.21450945\n",
            "Iteration 1939, loss = 0.21437854\n",
            "Iteration 1940, loss = 0.21424792\n",
            "Iteration 1941, loss = 0.21411760\n",
            "Iteration 1942, loss = 0.21398756\n",
            "Iteration 1943, loss = 0.21385782\n",
            "Iteration 1944, loss = 0.21372838\n",
            "Iteration 1945, loss = 0.21359922\n",
            "Iteration 1946, loss = 0.21347035\n",
            "Iteration 1947, loss = 0.21334177\n",
            "Iteration 1948, loss = 0.21321348\n",
            "Iteration 1949, loss = 0.21308548\n",
            "Iteration 1950, loss = 0.21295777\n",
            "Iteration 1951, loss = 0.21283034\n",
            "Iteration 1952, loss = 0.21270319\n",
            "Iteration 1953, loss = 0.21257633\n",
            "Iteration 1954, loss = 0.21244976\n",
            "Iteration 1955, loss = 0.21232347\n",
            "Iteration 1956, loss = 0.21219746\n",
            "Iteration 1957, loss = 0.21207173\n",
            "Iteration 1958, loss = 0.21194628\n",
            "Iteration 1959, loss = 0.21182111\n",
            "Iteration 1960, loss = 0.21169622\n",
            "Iteration 1961, loss = 0.21157161\n",
            "Iteration 1962, loss = 0.21144727\n",
            "Iteration 1963, loss = 0.21132322\n",
            "Iteration 1964, loss = 0.21119944\n",
            "Iteration 1965, loss = 0.21107593\n",
            "Iteration 1966, loss = 0.21095270\n",
            "Iteration 1967, loss = 0.21082974\n",
            "Iteration 1968, loss = 0.21070706\n",
            "Iteration 1969, loss = 0.21058465\n",
            "Iteration 1970, loss = 0.21046251\n",
            "Iteration 1971, loss = 0.21034064\n",
            "Iteration 1972, loss = 0.21021904\n",
            "Iteration 1973, loss = 0.21009771\n",
            "Iteration 1974, loss = 0.20997665\n",
            "Iteration 1975, loss = 0.20985586\n",
            "Iteration 1976, loss = 0.20973534\n",
            "Iteration 1977, loss = 0.20961508\n",
            "Iteration 1978, loss = 0.20949508\n",
            "Iteration 1979, loss = 0.20937536\n",
            "Iteration 1980, loss = 0.20925589\n",
            "Iteration 1981, loss = 0.20913669\n",
            "Iteration 1982, loss = 0.20901776\n",
            "Iteration 1983, loss = 0.20889908\n",
            "Iteration 1984, loss = 0.20878067\n",
            "Iteration 1985, loss = 0.20866252\n",
            "Iteration 1986, loss = 0.20854463\n",
            "Iteration 1987, loss = 0.20842699\n",
            "Iteration 1988, loss = 0.20830962\n",
            "Iteration 1989, loss = 0.20819250\n",
            "Iteration 1990, loss = 0.20807564\n",
            "Iteration 1991, loss = 0.20795904\n",
            "Iteration 1992, loss = 0.20784270\n",
            "Iteration 1993, loss = 0.20772661\n",
            "Iteration 1994, loss = 0.20761077\n",
            "Iteration 1995, loss = 0.20749519\n",
            "Iteration 1996, loss = 0.20737986\n",
            "Iteration 1997, loss = 0.20726478\n",
            "Iteration 1998, loss = 0.20714996\n",
            "Iteration 1999, loss = 0.20703538\n",
            "Iteration 2000, loss = 0.20692106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZjI6jfxVbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7dd53281-da45-4975-d372-f2af019fd5d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5538ed5ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAkFuohIlFCgi4hECQW6iEiU+P/ViGjmfDI6aQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}